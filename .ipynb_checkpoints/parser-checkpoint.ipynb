{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the libraries\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WordPunctTokenizer, MWETokenizer\n",
    "# import PyPDF2\n",
    "# for pdf to txt\n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import TextConverter\n",
    "from io import StringIO\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import openpyxl as px\n",
    "import docx2txt\n",
    "import constants as cs\n",
    "import string\n",
    "# import utils\n",
    "import pprint\n",
    "from spacy.matcher import matcher\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# from constants import STOPWORDS\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Name\", \"Email\", \"Phone\", \"Education\", \"Experience\", \"Skills\"]\n",
    "resume_data={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResumeProcessing\\Resume\\Mrunalresume.pdf\n"
     ]
    }
   ],
   "source": [
    "resume_file=input('Enter resume: ')\n",
    "print(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResumeProcessing\\Resume\\Mrunalresume.pdf\n"
     ]
    }
   ],
   "source": [
    "print(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract txt from pdf\n",
    "# pdf_path=\"code\\Resume.pdf\"\n",
    "def pdf_to_txt(pdf_path):\n",
    "    resource_manager = PDFResourceManager(caching=True)\n",
    "    \n",
    "    # create a string object that will contain the final text the representation of the pdf. \n",
    "    out_text = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laParams = LAParams()\n",
    "    \n",
    "    # Create a TextConverter Object:\n",
    "    text_converter = TextConverter(resource_manager, out_text, laparams=laParams)\n",
    "    fp = open(pdf_path, 'rb')\n",
    "    #Create a PDF interpreter object\n",
    "\n",
    "    interpreter = PDFPageInterpreter(resource_manager, text_converter)\n",
    "    \n",
    "    # We are going to process the content of each page of the original PDF File    \n",
    "    for page in PDFPage.get_pages(fp, pagenos=set(), maxpages=0, password=\"\", caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    \n",
    "    # Retrieve the entire contents of the “file” at any time \n",
    "    text = out_text.getvalue()\n",
    "\n",
    "    # Closing all the ressources we previously opened\n",
    "\n",
    "    fp.close()\n",
    "    text_converter.close()\n",
    "    out_text.close()\n",
    "    \n",
    "    return text\n",
    "# retext = pdf_to_txt(pdf_path)\n",
    "# print(pdf_to_txt(pdf_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract txt from\n",
    "# docx_path=\"Resume.docx\"\n",
    "def docx_to_txt(docx_path):\n",
    "    temp = docx2txt.process(docx_path)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)\n",
    "# print(docx_to_txt(docx_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ResumeProcessing\\\\Resume\\\\Mrunalresume', '.pdf')\n",
      "File Name:  ResumeProcessing\\Resume\\Mrunalresume\n",
      "File Extension:  .pdf\n"
     ]
    }
   ],
   "source": [
    "split_tup = os.path.splitext(resume_file)\n",
    "print(split_tup)\n",
    "  \n",
    "# extract the file name and extension\n",
    "file_name = split_tup[0]\n",
    "file_extension = split_tup[1]\n",
    "  \n",
    "print(\"File Name: \", file_name)\n",
    "print(\"File Extension: \", file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrunal Kulkarni\n",
      "Navi Mumbai\n",
      "9423627124 | maheshkulkarni01121@gmail.com\n",
      "LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni\n",
      "\n",
      "Career Objective\n",
      "To learn and expand my knowledge in the field of computer science, particularly in the area of\n",
      "data science through which I can earn a job which enables me to use my passion for A.I to grow\n",
      "the company as well as accomplish my self-development goals.\n",
      "\n",
      "Education Qualification\n",
      "\n",
      "● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with\n",
      "\n",
      "aggregate of 9.37.\n",
      "\n",
      "Sr.no\n",
      "\n",
      "Qualification\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "B.tech\n",
      "\n",
      "HSC\n",
      "\n",
      "SSC\n",
      "\n",
      "9.58\n",
      "\n",
      "9.70\n",
      "\n",
      "9.48\n",
      "\n",
      "8.90\n",
      "\n",
      "9.14\n",
      "\n",
      "9.45\n",
      "\n",
      "Grade\n",
      "\n",
      "Sem 1\n",
      "\n",
      "Sem 2\n",
      "\n",
      "Sem 3\n",
      "\n",
      "Sem 4\n",
      "\n",
      "Sem 5\n",
      "\n",
      "Sem 6\n",
      "\n",
      "88.2%\n",
      "\n",
      "94.4%\n",
      "\n",
      "University/board\n",
      "\n",
      "Mumbai University\n",
      "\n",
      "CBSE\n",
      "\n",
      "CBSE\n",
      "\n",
      "Technical Skills\n",
      "\n",
      "● Python programming skills\n",
      "● Machine learning\n",
      "● Natural Language Processing(BERT , Pegasus)\n",
      "● C programming skills\n",
      "● Web development (HTML,CSS,PHP)\n",
      "\n",
      "\fProjects Done\n",
      "\n",
      "● Teams meeting summarization website using Pegasus model of Hugging Face\n",
      "● Sentiment analysis using BERT\n",
      "● Book recommendation website using Python,HTML\n",
      "● Online Book review website\n",
      "● Log book report generation website using PHP\n",
      "\n",
      "Courses Done\n",
      "\n",
      "● Fine tuning BERT for text classification using Python Tensorflow by Coursera\n",
      "● Demystifying Networking course by IIT Bombay NPTEL\n",
      "● Python Course by Kaggle\n",
      "● C programming certificate by Spoken Tutorial\n",
      "\n",
      "Internships\n",
      "\n",
      "● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built\n",
      "a log book report generation website using PHP , Codeigniter using the existing database\n",
      "\n",
      "Achievements\n",
      "\n",
      "● Reached Semi-finals of Deep Blue Season 8\n",
      "\n",
      "Extra Curriculars\n",
      "\n",
      "● Content writing awards in College\n",
      "● Video Editing for social media\n",
      "\n",
      "Hobby\n",
      "\n",
      "● Story writing\n",
      "● Reading\n",
      "● Video editing\n",
      "\n",
      "Personal Details\n",
      "\n",
      "Name: Mrunal Mahesh Kulkarni\n",
      "Address: BARC colony ,Tarpur\n",
      "Phone No: 9423627124\n",
      "Languages known: English, Hindi, Marathi\n",
      "\n",
      "Declaration: I hereby declare that the above information is true to my knowledge.\n",
      "\n",
      "Date: 3/8/23\n",
      "Place: New Panvel\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "# detect file extension and call above functions accordingly\n",
    "def extract_text(file_path, extension):\n",
    "    '''\n",
    "    Wrapper function to detect the file extension and call text extraction function accordingly\n",
    "\n",
    "    :param file_path: path of file of which text is to be extracted\n",
    "    :param extension: extension of file `file_name`\n",
    "    '''\n",
    "    text = ''\n",
    "    if extension == '.pdf':\n",
    "        for page in pdf_to_txt(file_path):\n",
    "            text += '' + page\n",
    "    elif extension == '.docx' or extension == '.doc':\n",
    "        text = docx_to_txt(file_path)\n",
    "    return text\n",
    "Text= extract_text(resume_file,file_extension)\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Of the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121', '@', 'gmail.com', 'LinkedIn', ':', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science', ',', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A.I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self-development', 'goals', '.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9.37', '.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2', '%', '94.4', '%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing', '(', 'BERT', ',', 'Pegasus', ')', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(', 'HTML', ',', 'CSS', ',', 'PHP', ')', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python', ',', 'HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center', ',', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name', ':', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address', ':', 'BARC', 'colony', ',', 'Tarpur', 'Phone', 'No', ':', '9423627124', 'Languages', 'known', ':', 'English', ',', 'Hindi', ',', 'Marathi', 'Declaration', ':', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge', '.', 'Date', ':', '3/8/23', 'Place', ':', 'New', 'Panvel']\n",
      "TreebankWordTokenizer ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121', '@', 'gmail.com', 'LinkedIn', ':', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science', ',', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A.I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self-development', 'goals.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9.37.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2', '%', '94.4', '%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing', '(', 'BERT', ',', 'Pegasus', ')', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(', 'HTML', ',', 'CSS', ',', 'PHP', ')', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python', ',', 'HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center', ',', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name', ':', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address', ':', 'BARC', 'colony', ',', 'Tarpur', 'Phone', 'No', ':', '9423627124', 'Languages', 'known', ':', 'English', ',', 'Hindi', ',', 'Marathi', 'Declaration', ':', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge.', 'Date', ':', '3/8/23', 'Place', ':', 'New', 'Panvel']\n",
      "WordPunctTokenizer ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121', '@', 'gmail', '.', 'com', 'LinkedIn', ':', 'www', '.', 'linkedin', '.', 'com', '/', 'in', '/', 'mrunal', '-', 'mahesh', '-', 'kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science', ',', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A', '.', 'I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self', '-', 'development', 'goals', '.', 'Education', 'Qualification', '●', 'Pursuing', 'B', '.', 'Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9', '.', '37', '.', 'Sr', '.', 'no', 'Qualification', '1', '2', '3', 'B', '.', 'tech', 'HSC', 'SSC', '9', '.', '58', '9', '.', '70', '9', '.', '48', '8', '.', '90', '9', '.', '14', '9', '.', '45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88', '.', '2', '%', '94', '.', '4', '%', 'University', '/', 'board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing', '(', 'BERT', ',', 'Pegasus', ')', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(', 'HTML', ',', 'CSS', ',', 'PHP', ')', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python', ',', 'HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center', ',', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi', '-', 'finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name', ':', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address', ':', 'BARC', 'colony', ',', 'Tarpur', 'Phone', 'No', ':', '9423627124', 'Languages', 'known', ':', 'English', ',', 'Hindi', ',', 'Marathi', 'Declaration', ':', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge', '.', 'Date', ':', '3', '/', '8', '/', '23', 'Place', ':', 'New', 'Panvel']\n",
      "MWETokenizer ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121@gmail.com', 'LinkedIn:', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science,', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A.I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self-development', 'goals.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9.37.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2%', '94.4%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing(BERT', ',', 'Pegasus)', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(HTML,CSS,PHP)', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python,HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center,', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name:', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address:', 'BARC', 'colony', ',Tarpur', 'Phone', 'No:', '9423627124', 'Languages', 'known:', 'English,', 'Hindi,', 'Marathi', 'Declaration:', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge.', 'Date:', '3/8/23', 'Place:', 'New', 'Panvel']\n",
      "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni Career Objective To learn and expand my knowledge in the field of computer science, particularly in the area of data science through which I can earn a job which enables me to use my passion for A.I to grow the company as well as accomplish my self-development goals. Education Qualification ● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with aggregate of 9.37. Sr.no Qualification 1 2 3 B.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2% 94.4% University/board Mumbai University CBSE CBSE Technical Skills ● Python programming skills ● Machine learning ● Natural Language Processing(BERT , Pegasus) ● C programming skills ● Web development (HTML,CSS,PHP) Projects Done ● Teams meeting summarization website using Pegasus model of Hugging Face ● Sentiment analysis using BERT ● Book recommendation website using Python,HTML ● Online Book review website ● Log book report generation website using PHP Courses Done ● Fine tuning BERT for text classification using Python Tensorflow by Coursera ● Demystifying Networking course by IIT Bombay NPTEL ● Python Course by Kaggle ● C programming certificate by Spoken Tutorial Internships ● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built a log book report generation website using PHP , Codeigniter using the existing database Achievements ● Reached Semi-finals of Deep Blue Season 8 Extra Curriculars ● Content writing awards in College ● Video Editing for social media Hobby ● Story writing ● Reading ● Video editing Personal Details Name: Mrunal Mahesh Kulkarni Address: BARC colony ,Tarpur Phone No: 9423627124 Languages known: English, Hindi, Marathi Declaration: I hereby declare that the above information is true to my knowledge. Date: 3/8/23 Place: New Panvel\n"
     ]
    }
   ],
   "source": [
    "#word tokenize\n",
    "print(\"word_tokenize\",word_tokenize(Text))\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(\"TreebankWordTokenizer\",tokenizer.tokenize(Text))\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(\"WordPunctTokenizer\",tokenizer.tokenize(Text) )\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "mtokenizer = MWETokenizer([('athale20comp@studen', 't.mes.ac.in'), ('+91', '-', '8605131403')],separator='')\n",
    "aftertoken = mtokenizer.tokenize(Text.split())\n",
    "print(\"MWETokenizer\",aftertoken)\n",
    "aftertoken=(' '.join(aftertoken))\n",
    "print(aftertoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mrunal Kulkarni\\nNavi Mumbai\\n9423627124 | maheshkulkarni01121@gmail.com\\nLinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni\\n\\nCareer Objective\\nTo learn and expand my knowledge in the field of computer science, particularly in the area of\\ndata science through which I can earn a job which enables me to use my passion for A.I to grow\\nthe company as well as accomplish my self-development goals.', 'Education Qualification\\n\\n● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with\\n\\naggregate of 9.37.', 'Sr.no\\n\\nQualification\\n\\n1\\n\\n2\\n\\n3\\n\\nB.tech\\n\\nHSC\\n\\nSSC\\n\\n9.58\\n\\n9.70\\n\\n9.48\\n\\n8.90\\n\\n9.14\\n\\n9.45\\n\\nGrade\\n\\nSem 1\\n\\nSem 2\\n\\nSem 3\\n\\nSem 4\\n\\nSem 5\\n\\nSem 6\\n\\n88.2%\\n\\n94.4%\\n\\nUniversity/board\\n\\nMumbai University\\n\\nCBSE\\n\\nCBSE\\n\\nTechnical Skills\\n\\n● Python programming skills\\n● Machine learning\\n● Natural Language Processing(BERT , Pegasus)\\n● C programming skills\\n● Web development (HTML,CSS,PHP)\\n\\n\\x0cProjects Done\\n\\n● Teams meeting summarization website using Pegasus model of Hugging Face\\n● Sentiment analysis using BERT\\n● Book recommendation website using Python,HTML\\n● Online Book review website\\n● Log book report generation website using PHP\\n\\nCourses Done\\n\\n● Fine tuning BERT for text classification using Python Tensorflow by Coursera\\n● Demystifying Networking course by IIT Bombay NPTEL\\n● Python Course by Kaggle\\n● C programming certificate by Spoken Tutorial\\n\\nInternships\\n\\n● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built\\na log book report generation website using PHP , Codeigniter using the existing database\\n\\nAchievements\\n\\n● Reached Semi-finals of Deep Blue Season 8\\n\\nExtra Curriculars\\n\\n● Content writing awards in College\\n● Video Editing for social media\\n\\nHobby\\n\\n● Story writing\\n● Reading\\n● Video editing\\n\\nPersonal Details\\n\\nName: Mrunal Mahesh Kulkarni\\nAddress: BARC colony ,Tarpur\\nPhone No: 9423627124\\nLanguages known: English, Hindi, Marathi\\n\\nDeclaration: I hereby declare that the above information is true to my knowledge.', 'Date: 3/8/23\\nPlace: New Panvel']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  filter_text(Text,lowerFlag=False,upperFlag=False,numberFlag=False,htmlFlag=False,urlFlag=False,punctFlag=False,spaceFlag=False,charFlag=False,bulletFlag=False,hashtagFlag=False,emojiFlag=False):\n",
    "    if lowerFlag:\n",
    "      Text = Text.lower()\n",
    "\n",
    "    if upperFlag:\n",
    "      Text = Text.upper()\n",
    "\n",
    "    if numberFlag:\n",
    "      import re\n",
    "      Text = re.sub(r\"\\d+\", '', Text)\n",
    "\n",
    "    if htmlFlag:\n",
    "      import re\n",
    "      Text = re.sub(r'<[^>]*>', '', Text)\n",
    "\n",
    "    if urlFlag:\n",
    "      import re\n",
    "      Text = re.sub(r'(https?|ftp|www)\\S+', '', Text)\n",
    "\n",
    "    if punctFlag:\n",
    "      import re\n",
    "      import string\n",
    "      exclist = string.punctuation #removes [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "      # remove punctuations and digits from oldtext\n",
    "      table_ = Text.maketrans('', '', exclist)\n",
    "      Text = Text.translate(table_)\n",
    "\n",
    "    if spaceFlag:\n",
    "      import re\n",
    "      Text = re.sub(' +',\" \",Text).strip()\n",
    "\n",
    "    if hashtagFlag:\n",
    "      import re\n",
    "      Text = re.sub(' +',\"#\",Text).strip()\n",
    "      \n",
    "    if charFlag:\n",
    "      import re\n",
    "      Text = re.sub('\\n', ' ', Text)\n",
    "      \n",
    "    if bulletFlag:\n",
    "      import re\n",
    "      Text = re.sub('●','',Text)\n",
    "      Text = re.sub('★','',Text)\n",
    "      Text = re.sub('|','',Text)\n",
    "      # Text = re.sub(([0-9|A-Z]+[\\.|\\)]|)\\s+,'', Text)\n",
    "    \n",
    "    if emojiFlag:\n",
    "      import emoji\n",
    "      Text = emoji.sub(' +',\"#\",Text).strip()\n",
    "      pass\n",
    "\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni Career Objective To learn and expand my knowledge in the field of computer science, particularly in the area of data science through which I can earn a job which enables me to use my passion for A.I to grow the company as well as accomplish my self-development goals. Education Qualification  Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with aggregate of 9.37. Sr.no Qualification 1 2 3 B.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2% 94.4% University/board Mumbai University CBSE CBSE Technical Skills  Python programming skills  Machine learning  Natural Language Processing(BERT , Pegasus)  C programming skills  Web development (HTML,CSS,PHP) Projects Done  Teams meeting summarization website using Pegasus model of Hugging Face  Sentiment analysis using BERT  Book recommendation website using Python,HTML  Online Book review website  Log book report generation website using PHP Courses Done  Fine tuning BERT for text classification using Python Tensorflow by Coursera  Demystifying Networking course by IIT Bombay NPTEL  Python Course by Kaggle  C programming certificate by Spoken Tutorial Internships  Completed one month training at Bhabha Atomic Research Center, Tarapur where I built a log book report generation website using PHP , Codeigniter using the existing database Achievements  Reached Semi-finals of Deep Blue Season 8 Extra Curriculars  Content writing awards in College  Video Editing for social media Hobby  Story writing  Reading  Video editing Personal Details Name: Mrunal Mahesh Kulkarni Address: BARC colony ,Tarpur Phone No: 9423627124 Languages known: English, Hindi, Marathi Declaration: I hereby declare that the above information is true to my knowledge. Date: 3/8/23 Place: New Panvel\n"
     ]
    }
   ],
   "source": [
    "filteredtxt = filter_text(aftertoken, bulletFlag=True, charFlag=True)\n",
    "print(filteredtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'these', 'a', 'wouldn', \"shan't\", \"mightn't\", 's', 'that', \"mustn't\", \"weren't\", 'down', \"isn't\", 'doesn', \"wouldn't\", 'then', 'our', 'the', 'your', 'only', \"shouldn't\", \"haven't\", 'than', 'didn', \"she's\", 'whom', 'does', 'which', 'over', 'nor', 'theirs', 'doing', 'will', 'yours', 'should', 'mightn', 'have', 'who', \"hadn't\", 'he', 'hadn', 'o', 'but', \"that'll\", \"wasn't\", 'same', 'couldn', \"you'd\", 'her', 'or', \"doesn't\", \"should've\", 'was', 're', 'me', 'some', 'how', 'until', 'itself', 'can', 'against', 'an', 'into', 'herself', 'below', 'other', 'is', 'we', 'it', 'shouldn', 'own', 'himself', 'yourself', \"it's\", 'off', 'both', 'with', 'ain', 'aren', 'm', 'shan', 'him', 'she', 'd', 'by', 'not', 'am', 'his', 'further', 'don', 'hasn', 'at', 'in', 'yourselves', 'each', 'do', \"hasn't\", 'now', 'there', 'ourselves', 'needn', 'were', 'above', \"you'll\", 'being', 'to', 'and', \"didn't\", 'hers', 'did', 'because', 'here', 'from', 'been', 'mustn', 'any', \"aren't\", 'just', 'themselves', 'up', 'out', 'between', 'where', 'why', 'such', 'this', 'no', 'weren', 'so', 'them', 'all', 'those', 'for', 'of', 'on', 'about', 'once', 've', 'you', 'isn', 'if', 'be', \"you've\", 'i', 'll', 'when', 'too', \"don't\", 'myself', 'after', 'having', 'haven', 'has', 'through', 'they', 'what', 'as', \"won't\", 'wasn', 'y', \"couldn't\", 'ma', 'their', 't', 'are', \"you're\", 'before', 'few', 'again', 'during', 'its', 'under', 'very', 'while', 'most', 'more', 'had', \"needn't\", 'won', 'my'}\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopWords recognized in the given sentence: ['a', 'that', 'the', 'which', 'me', 'can', 'is', 'with', 'by', 'at', 'in', 'above', 'to', 'and', 'where', 'for', 'of', 'through', 'as', 'my'] \n",
      "\n",
      "After removing the recognized stopwords, the Tokens of sentence is: ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121@gmail.com', 'LinkedIn:', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'expand', 'knowledge', 'field', 'computer', 'science,', 'particularly', 'area', 'data', 'science', 'I', 'earn', 'job', 'enables', 'use', 'passion', 'A.I', 'grow', 'company', 'well', 'accomplish', 'self-development', 'goals.', 'Education', 'Qualification', 'Pursuing', 'B.Tech', 'semester', '7', 'computer', 'science', 'Pillai', 'College', 'Engineering', 'aggregate', '9.37.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2%', '94.4%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', 'Python', 'programming', 'skills', 'Machine', 'learning', 'Natural', 'Language', 'Processing(BERT', ',', 'Pegasus)', 'C', 'programming', 'skills', 'Web', 'development', '(HTML,CSS,PHP)', 'Projects', 'Done', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'Hugging', 'Face', 'Sentiment', 'analysis', 'using', 'BERT', 'Book', 'recommendation', 'website', 'using', 'Python,HTML', 'Online', 'Book', 'review', 'website', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', 'Fine', 'tuning', 'BERT', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'Coursera', 'Demystifying', 'Networking', 'course', 'IIT', 'Bombay', 'NPTEL', 'Python', 'Course', 'Kaggle', 'C', 'programming', 'certificate', 'Spoken', 'Tutorial', 'Internships', 'Completed', 'one', 'month', 'training', 'Bhabha', 'Atomic', 'Research', 'Center,', 'Tarapur', 'I', 'built', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'existing', 'database', 'Achievements', 'Reached', 'Semi-finals', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', 'Content', 'writing', 'awards', 'College', 'Video', 'Editing', 'social', 'media', 'Hobby', 'Story', 'writing', 'Reading', 'Video', 'editing', 'Personal', 'Details', 'Name:', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address:', 'BARC', 'colony', ',Tarpur', 'Phone', 'No:', '9423627124', 'Languages', 'known:', 'English,', 'Hindi,', 'Marathi', 'Declaration:', 'I', 'hereby', 'declare', 'information', 'true', 'knowledge.', 'Date:', '3/8/23', 'Place:', 'New', 'Panvel']\n",
      "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni Career Objective To learn expand knowledge field computer science, particularly area data science I earn job enables use passion A.I grow company well accomplish self-development goals. Education Qualification Pursuing B.Tech semester 7 computer science Pillai College Engineering aggregate 9.37. Sr.no Qualification 1 2 3 B.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2% 94.4% University/board Mumbai University CBSE CBSE Technical Skills Python programming skills Machine learning Natural Language Processing(BERT , Pegasus) C programming skills Web development (HTML,CSS,PHP) Projects Done Teams meeting summarization website using Pegasus model Hugging Face Sentiment analysis using BERT Book recommendation website using Python,HTML Online Book review website Log book report generation website using PHP Courses Done Fine tuning BERT text classification using Python Tensorflow Coursera Demystifying Networking course IIT Bombay NPTEL Python Course Kaggle C programming certificate Spoken Tutorial Internships Completed one month training Bhabha Atomic Research Center, Tarapur I built log book report generation website using PHP , Codeigniter using existing database Achievements Reached Semi-finals Deep Blue Season 8 Extra Curriculars Content writing awards College Video Editing social media Hobby Story writing Reading Video editing Personal Details Name: Mrunal Mahesh Kulkarni Address: BARC colony ,Tarpur Phone No: 9423627124 Languages known: English, Hindi, Marathi Declaration: I hereby declare information true knowledge. Date: 3/8/23 Place: New Panvel\n"
     ]
    }
   ],
   "source": [
    "mtokenizer = MWETokenizer([('athale20comp@studen', 't.mes.ac.in'), ('+91', '-', '8605131403')],separator='')\n",
    "word_tokens = mtokenizer.tokenize(filteredtxt.split())\n",
    "\n",
    "# print(ex_text)\n",
    "\n",
    "stop = [w for w in stop_words if w in word_tokens]\n",
    "print(\"StopWords recognized in the given sentence:\", stop,\"\\n\")\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(\"After removing the recognized stopwords, the Tokens of sentence is:\", filtered_sentence)\n",
    "filtered_data=(' '.join(filtered_sentence))\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mrunal, Kulkarni, Navi, Mumbai, 9423627124, |, maheshkulkarni01121@gmail.com, LinkedIn, :, www.linkedin.com/in/mrunal-mahesh-kulkarni, Career, Objective, To, learn, expand, knowledge, field, computer, science, ,, particularly, area, data, science, I, earn, job, enables, use, passion, A.I, grow, company, well, accomplish, self, -, development, goals, ., Education, Qualification, Pursuing, B.Tech, semester, 7, computer, science, Pillai, College, Engineering, aggregate, 9.37, ., Sr.no, Qualification, 1, 2, 3, B.tech, HSC, SSC, 9.58, 9.70, 9.48, 8.90, 9.14, 9.45, Grade, Sem, 1, Sem, 2, Sem, 3, Sem, 4, Sem, 5, Sem, 6, 88.2, %, 94.4, %, University, /, board, Mumbai, University, CBSE, CBSE, Technical, Skills, Python, programming, skills, Machine, learning, Natural, Language, Processing(BERT, ,, Pegasus, ), C, programming, skills, Web, development, (, HTML, ,, CSS, ,, PHP, ), Projects, Done, Teams, meeting, summarization, website, using, Pegasus, model, Hugging, Face, Sentiment, analysis, using, BERT, Book, recommendation, website, using, Python, ,, HTML, Online, Book, review, website, Log, book, report, generation, website, using, PHP, Courses, Done, Fine, tuning, BERT, text, classification, using, Python, Tensorflow, Coursera, Demystifying, Networking, course, IIT, Bombay, NPTEL, Python, Course, Kaggle, C, programming, certificate, Spoken, Tutorial, Internships, Completed, one, month, training, Bhabha, Atomic, Research, Center, ,, Tarapur, I, built, log, book, report, generation, website, using, PHP, ,, Codeigniter, using, existing, database, Achievements, Reached, Semi, -, finals, Deep, Blue, Season, 8, Extra, Curriculars, Content, writing, awards, College, Video, Editing, social, media, Hobby, Story, writing, Reading, Video, editing, Personal, Details, Name, :, Mrunal, Mahesh, Kulkarni, Address, :, BARC, colony, ,, Tarpur, Phone, No, :, 9423627124, Languages, known, :, English, ,, Hindi, ,, Marathi, Declaration, :, I, hereby, declare, information, true, knowledge, ., Date, :, 3/8/23, Place, :, New, Panvel]\n",
      "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn : www.linkedin.com/in/mrunal-mahesh-kulkarni career objective to learn expand knowledge field computer science , particularly area datum science I earn job enable use passion A.I grow company well accomplish self - development goal . Education Qualification Pursuing B.Tech semester 7 computer science Pillai College Engineering aggregate 9.37 . Sr.no Qualification 1 2 3 b.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2 % 94.4 % University / board Mumbai University CBSE CBSE Technical Skills Python programming skill machine learn Natural Language Processing(BERT , Pegasus ) c programming skill web development ( HTML , CSS , PHP ) project do Teams meet summarization website use Pegasus model Hugging Face Sentiment analysis use BERT Book recommendation website use Python , HTML Online Book review website Log book report generation website use PHP Courses do fine tune BERT text classification use Python Tensorflow Coursera Demystifying Networking course IIT Bombay NPTEL Python course Kaggle C programming certificate Spoken Tutorial Internships complete one month train Bhabha Atomic Research Center , Tarapur I build log book report generation website use PHP , Codeigniter use exist database achievement Reached Semi - final Deep Blue Season 8 Extra Curriculars Content write award College Video Editing social medium Hobby Story write Reading Video edit Personal Details Name : Mrunal Mahesh Kulkarni Address : barc colony , Tarpur Phone no : 9423627124 language know : English , Hindi , Marathi Declaration : I hereby declare information true knowledge . date : 3/8/23 place : New Panvel\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(filtered_data)\n",
    "\n",
    "# Create list of tokens from given string\n",
    "tokens = []\n",
    "for token in doc:\n",
    "\ttokens.append(token)\n",
    "\n",
    "print(tokens)\n",
    "#> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n",
    "\n",
    "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "print(lemmatized_sentence)\n",
    "#> the bat see the cat with good stripe hang upside down by -PRON- foot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'constants' has no attribute 'RESUME_SECTIONS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thale\\OneDrive\\Documents\\Avanti\\SEM 6\\resume analysis\\Resume-Analysis\\parser.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# entity_key = False\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# for entity in entities.keys():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m#     sub_entities = {}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m#     if entity not in entities.keys():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m#         entities[entity] = None \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m entities\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m pprint\u001b[39m.\u001b[39mpprint(extract_entity_sections(Text))\n",
      "\u001b[1;32mc:\\Users\\thale\\OneDrive\\Documents\\Avanti\\SEM 6\\resume analysis\\Resume-Analysis\\parser.ipynb Cell 23\u001b[0m in \u001b[0;36mextract_entity_sections\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     p_key \u001b[39m=\u001b[39m phrase\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     p_key \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(phrase\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39msplit()) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m(cs\u001b[39m.\u001b[39;49mRESUME_SECTIONS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/parser.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     p_key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(p_key)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'constants' has no attribute 'RESUME_SECTIONS'"
     ]
    }
   ],
   "source": [
    "def extract_entity_sections(text):\n",
    "    '''\n",
    "    Helper function to extract all the raw text from sections of resume\n",
    "\n",
    "    :param text: Raw text of resume\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    RESUME_SECTIONS = [\n",
    "                    'accomplishments',\n",
    "                    'achievements',\n",
    "                    'experience',\n",
    "                    'education',\n",
    "                    'interests',\n",
    "                    'hobbies',\n",
    "                    'projects',\n",
    "                    'professional experience',\n",
    "                    'experience',\n",
    "                    'publications',\n",
    "                    'skills',\n",
    "                ]\n",
    "    text_split = [i.strip() for i in text.split('\\n')]\n",
    "    # sections_in_resume = [i for i in text_split if i.lower() in sections]\n",
    "    entities = {}\n",
    "    key = False\n",
    "    for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(cs.RESUME_SECTIONS)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in RESUME_SECTIONS:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "    \n",
    "    # entity_key = False\n",
    "    # for entity in entities.keys():\n",
    "    #     sub_entities = {}\n",
    "    #     for entry in entities[entity]:\n",
    "    #         if u'\\u2022' not in entry:\n",
    "    #             sub_entities[entry] = []\n",
    "    #             entity_key = entry\n",
    "    #         elif entity_key:\n",
    "    #             sub_entities[entity_key].append(entry)\n",
    "    #     entities[entity] = sub_entities\n",
    "\n",
    "    # # pprint.pprint(entities)\n",
    "\n",
    "    # make entities that are not found None\n",
    "    # for entity in cs.RESUME_SECTIONS:\n",
    "    #     if entity not in entities.keys():\n",
    "    #         entities[entity] = None \n",
    "    \n",
    "    return entities\n",
    "pprint.pprint(extract_entity_sections(Text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-mail:  athale20comp@student.mes.ac.in\n"
     ]
    }
   ],
   "source": [
    "def extract_email(text):\n",
    "    '''\n",
    "    Helper function to extract email id from text\n",
    "\n",
    "    :param text: plain text extracted from resume file\n",
    "    '''\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "print(\"E-mail: \",extract_email(filtered_data))\n",
    "resume_data[titles[1]]=extract_email(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Avanti Makarand Thale\n",
      "Name:  EDUCATION QUALIFICATION University\n",
      "Name:  PRESENT B.Tech Computer\n",
      "Name:  B.Tech Computer Science\n",
      "Name:  Computer Science Specialization\n",
      "Name:  Science Specialization Data\n",
      "Name:  Specialization Data Science\n",
      "Name:  Data Science Natural\n",
      "Name:  Science Natural Language\n",
      "Name:  Natural Language Processing\n",
      "Name:  Language Processing Pillai\n",
      "Name:  Processing Pillai College\n",
      "Name:  Panvel Jindal Mount\n",
      "Name:  Jindal Mount Litera\n",
      "Name:  Mount Litera Zee\n",
      "Name:  Litera Zee School\n",
      "Name:  XII Science CBSE\n",
      "Name:  Jindal Vidya Mandir\n",
      "Name:  Vidya Mandir Salav\n",
      "Name:  CBSE SKILLS Leadership\n",
      "Name:  SKILLS Leadership Problem\n",
      "Name:  Leadership Problem Solving\n",
      "Name:  Problem Solving Teamwork\n",
      "Name:  Solving Teamwork PROGRAMMING\n",
      "Name:  Teamwork PROGRAMMING LANGUAGES\n",
      "Name:  PROGRAMMING LANGUAGES Python\n",
      "Name:  LANGUAGES Python Java\n",
      "Name:  Python Java Javascript\n",
      "Name:  Java Javascript HTML\n",
      "Name:  Javascript HTML CSS\n",
      "Name:  HTML CSS C\n",
      "Name:  CSS C Django\n",
      "Name:  C Django INTERESTS\n",
      "Name:  Project Deep Blue\n",
      "Name:  Deep Blue Participated\n",
      "Name:  Blue Participated Project\n",
      "Name:  Participated Project Deep\n",
      "Name:  Project Deep Blue\n",
      "Name:  Deep Blue Leader\n",
      "Name:  Support Vector Machine\n",
      "Name:  Vector Machine Algorithm\n",
      "Name:  Machine Algorithm Book\n",
      "Name:  Algorithm Book Review\n",
      "Name:  Book Review Site\n",
      "Name:  PHP Railway Reservation\n",
      "Name:  Railway Reservation System\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# english_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# text = '''\n",
    "# This is a sample text that contains the name Alex Smith who is one of the developers of this project.\n",
    "# You can also find the surname Jones here.\n",
    "# '''\n",
    "\n",
    "# spacy_parser = english_nlp(filtered_data)\n",
    "\n",
    "# for entity in new_nlp.ents:\n",
    "#     print(f'Found: {entity.text} of type: {entity.label_}')\n",
    "    \n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "new_matcher = Matcher(nlp.vocab)\n",
    "new_nlp = nlp(filtered_data)\n",
    "\n",
    "# def extract_name(text, matcher):\n",
    "#     '''\n",
    "#     Helper function to extract name from spacy nlp text\n",
    "\n",
    "#     :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "#     :param matcher: object of `spacy.matcher.Matcher`\n",
    "#     :return: string of full name\n",
    "#     '''\n",
    "pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "new_matcher.add('NAME',[pattern], on_match=None)\n",
    "matches = new_matcher(new_nlp)\n",
    "    \n",
    "for match_id, start, end in matches:\n",
    "    span = new_nlp[start:end]\n",
    "    print(\"Name: \",span)\n",
    "\n",
    "# print(extract_name(new_nlp, new_matcher))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: ▁Av, Label: LABEL_1\n",
      "Entity: anti, Label: LABEL_1\n",
      "Entity: ▁Maka, Label: LABEL_1\n",
      "Entity: rand, Label: LABEL_0\n",
      "Entity: ▁Tha, Label: LABEL_0\n",
      "Entity: le, Label: LABEL_0\n",
      "Entity: ▁at, Label: LABEL_1\n",
      "Entity: hale, Label: LABEL_1\n",
      "Entity: 20, Label: LABEL_0\n",
      "Entity: comp, Label: LABEL_0\n",
      "Entity: @, Label: LABEL_0\n",
      "Entity: stud, Label: LABEL_0\n",
      "Entity: en, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: t, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: mes, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: ac, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: in, Label: LABEL_0\n",
      "Entity: ▁+, Label: LABEL_0\n",
      "Entity: 91, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: -, Label: LABEL_0\n",
      "Entity: ▁86, Label: LABEL_0\n",
      "Entity: 05, Label: LABEL_0\n",
      "Entity: 13, Label: LABEL_0\n",
      "Entity: 14, Label: LABEL_0\n",
      "Entity: 03, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: CAR, Label: LABEL_0\n",
      "Entity: EER, Label: LABEL_0\n",
      "Entity: ▁O, Label: LABEL_0\n",
      "Entity: BJ, Label: LABEL_0\n",
      "Entity: ECT, Label: LABEL_0\n",
      "Entity: IVE, Label: LABEL_0\n",
      "Entity: ▁To, Label: LABEL_0\n",
      "Entity: ▁obtain, Label: LABEL_0\n",
      "Entity: ▁a, Label: LABEL_0\n",
      "Entity: ▁challenging, Label: LABEL_0\n",
      "Entity: ▁position, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁software, Label: LABEL_0\n",
      "Entity: ▁design, Label: LABEL_0\n",
      "Entity: ▁while, Label: LABEL_0\n",
      "Entity: ▁contributing, Label: LABEL_0\n",
      "Entity: ▁significantly, Label: LABEL_0\n",
      "Entity: ▁to, Label: LABEL_0\n",
      "Entity: ▁the, Label: LABEL_1\n",
      "Entity: ▁company, Label: LABEL_0\n",
      "Entity: ', Label: LABEL_0\n",
      "Entity: s, Label: LABEL_0\n",
      "Entity: ▁growth, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: ED, Label: LABEL_0\n",
      "Entity: UC, Label: LABEL_0\n",
      "Entity: ATION, Label: LABEL_0\n",
      "Entity: ▁Q, Label: LABEL_0\n",
      "Entity: UAL, Label: LABEL_0\n",
      "Entity: IF, Label: LABEL_0\n",
      "Entity: IC, Label: LABEL_0\n",
      "Entity: ATION, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁University, Label: LABEL_0\n",
      "Entity: ▁Of, Label: LABEL_0\n",
      "Entity: ▁Mumbai, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_0\n",
      "Entity: ▁Mumbai, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: —, Label: LABEL_0\n",
      "Entity: ▁Gra, Label: LABEL_0\n",
      "Entity: d, Label: LABEL_0\n",
      "Entity: uation, Label: LABEL_0\n",
      "Entity: ▁A, Label: LABEL_0\n",
      "Entity: UG, Label: LABEL_0\n",
      "Entity: ▁2020, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: –, Label: LABEL_0\n",
      "Entity: ▁PRE, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ENT, Label: LABEL_0\n",
      "Entity: ▁B, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: Tech, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁Computer, Label: LABEL_0\n",
      "Entity: ▁Science, Label: LABEL_0\n",
      "Entity: ▁with, Label: LABEL_0\n",
      "Entity: ▁Special, Label: LABEL_0\n",
      "Entity: ization, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁Data, Label: LABEL_0\n",
      "Entity: ▁Science, Label: LABEL_1\n",
      "Entity: ▁and, Label: LABEL_0\n",
      "Entity: ▁Natural, Label: LABEL_0\n",
      "Entity: ▁Language, Label: LABEL_0\n",
      "Entity: ▁Processing, Label: LABEL_0\n",
      "Entity: ▁through, Label: LABEL_0\n",
      "Entity: ▁Pill, Label: LABEL_0\n",
      "Entity: ai, Label: LABEL_0\n",
      "Entity: ▁College, Label: LABEL_0\n",
      "Entity: ▁Of, Label: LABEL_0\n",
      "Entity: ▁Engineering, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_1\n",
      "Entity: ▁Pan, Label: LABEL_0\n",
      "Entity: vel, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Jin, Label: LABEL_0\n",
      "Entity: dal, Label: LABEL_0\n",
      "Entity: ▁Mount, Label: LABEL_0\n",
      "Entity: ▁Li, Label: LABEL_0\n",
      "Entity: tera, Label: LABEL_0\n",
      "Entity: ▁Ze, Label: LABEL_0\n",
      "Entity: e, Label: LABEL_0\n",
      "Entity: ▁School, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_0\n",
      "Entity: ▁Na, Label: LABEL_0\n",
      "Entity: got, Label: LABEL_0\n",
      "Entity: hane, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: —, Label: LABEL_0\n",
      "Entity: ▁H, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: C, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: MAR, Label: LABEL_0\n",
      "Entity: ▁2020, Label: LABEL_0\n",
      "Entity: ▁scored, Label: LABEL_0\n",
      "Entity: ▁8, Label: LABEL_0\n",
      "Entity: 4%, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁XI, Label: LABEL_0\n",
      "Entity: I, Label: LABEL_0\n",
      "Entity: ▁Science, Label: LABEL_0\n",
      "Entity: ▁CBS, Label: LABEL_0\n",
      "Entity: E, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: (, Label: LABEL_0\n",
      "Entity: PC, Label: LABEL_0\n",
      "Entity: MB, Label: LABEL_0\n",
      "Entity: ), Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Jin, Label: LABEL_0\n",
      "Entity: dal, Label: LABEL_0\n",
      "Entity: ▁Vid, Label: LABEL_1\n",
      "Entity: ya, Label: LABEL_1\n",
      "Entity: ▁Mand, Label: LABEL_1\n",
      "Entity: ir, Label: LABEL_0\n",
      "Entity: ▁Sal, Label: LABEL_0\n",
      "Entity: av, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_0\n",
      "Entity: ▁Mur, Label: LABEL_0\n",
      "Entity: ud, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: —, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ., Label: LABEL_0\n",
      "Entity: C, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: MAR, Label: LABEL_0\n",
      "Entity: ▁2018, Label: LABEL_0\n",
      "Entity: ▁Score, Label: LABEL_0\n",
      "Entity: d, Label: LABEL_0\n",
      "Entity: ▁9, Label: LABEL_0\n",
      "Entity: 4%, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁class, Label: LABEL_0\n",
      "Entity: ▁X, Label: LABEL_0\n",
      "Entity: ▁CBS, Label: LABEL_0\n",
      "Entity: E, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: K, Label: LABEL_0\n",
      "Entity: ILL, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Leadership, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Pro, Label: LABEL_1\n",
      "Entity: ble, Label: LABEL_1\n",
      "Entity: m, Label: LABEL_0\n",
      "Entity: ▁So, Label: LABEL_1\n",
      "Entity: lving, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Team, Label: LABEL_1\n",
      "Entity: work, Label: LABEL_0\n",
      "Entity: ▁PRO, Label: LABEL_0\n",
      "Entity: GRA, Label: LABEL_1\n",
      "Entity: MM, Label: LABEL_0\n",
      "Entity: ING, Label: LABEL_1\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: LAN, Label: LABEL_0\n",
      "Entity: GU, Label: LABEL_0\n",
      "Entity: AGE, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Python, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Java, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Java, Label: LABEL_0\n",
      "Entity: script, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁HTML, Label: LABEL_1\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁CSS, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁C, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Dj, Label: LABEL_0\n",
      "Entity: ango, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: INTER, Label: LABEL_0\n",
      "Entity: EST, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: Listen, Label: LABEL_0\n",
      "Entity: ing, Label: LABEL_0\n",
      "Entity: ▁music, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Reading, Label: LABEL_0\n",
      "Entity: ▁Books, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: ACH, Label: LABEL_0\n",
      "Entity: IE, Label: LABEL_0\n",
      "Entity: VE, Label: LABEL_0\n",
      "Entity: MENT, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Project, Label: LABEL_0\n",
      "Entity: ▁Deep, Label: LABEL_0\n",
      "Entity: ▁Blue, Label: LABEL_0\n",
      "Entity: ▁Part, Label: LABEL_0\n",
      "Entity: i, Label: LABEL_0\n",
      "Entity: cip, Label: LABEL_0\n",
      "Entity: ated, Label: LABEL_0\n",
      "Entity: ▁in, Label: LABEL_0\n",
      "Entity: ▁Project, Label: LABEL_0\n",
      "Entity: ▁Deep, Label: LABEL_0\n",
      "Entity: ▁Blue, Label: LABEL_0\n",
      "Entity: ▁as, Label: LABEL_0\n",
      "Entity: ▁the, Label: LABEL_0\n",
      "Entity: ▁Leader, Label: LABEL_0\n",
      "Entity: ▁of, Label: LABEL_0\n",
      "Entity: ▁the, Label: LABEL_0\n",
      "Entity: ▁team, Label: LABEL_0\n",
      "Entity: ▁and, Label: LABEL_0\n",
      "Entity: ▁worked, Label: LABEL_0\n",
      "Entity: ▁on, Label: LABEL_1\n",
      "Entity: ▁the, Label: LABEL_1\n",
      "Entity: ▁solution, Label: LABEL_0\n",
      "Entity: ▁for, Label: LABEL_1\n",
      "Entity: ▁Sum, Label: LABEL_1\n",
      "Entity: mar, Label: LABEL_1\n",
      "Entity: izing, Label: LABEL_1\n",
      "Entity: ▁the, Label: LABEL_1\n",
      "Entity: ▁teams, Label: LABEL_0\n",
      "Entity: ▁meeting, Label: LABEL_0\n",
      "Entity: ▁PRO, Label: LABEL_0\n",
      "Entity: J, Label: LABEL_0\n",
      "Entity: ECT, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Diabetes, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_1\n",
      "Entity: Detect, Label: LABEL_0\n",
      "Entity: ion, Label: LABEL_0\n",
      "Entity: ▁System, Label: LABEL_0\n",
      "Entity: ▁using, Label: LABEL_0\n",
      "Entity: ▁Python, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_1\n",
      "Entity: ▁Fla, Label: LABEL_0\n",
      "Entity: sk, Label: LABEL_0\n",
      "Entity: ▁Application, Label: LABEL_0\n",
      "Entity: ▁using, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: (, Label: LABEL_0\n",
      "Entity: S, Label: LABEL_0\n",
      "Entity: VM, Label: LABEL_0\n",
      "Entity: ), Label: LABEL_1\n",
      "Entity: ▁Support, Label: LABEL_0\n",
      "Entity: ▁Ve, Label: LABEL_0\n",
      "Entity: ctor, Label: LABEL_1\n",
      "Entity: ▁Machine, Label: LABEL_1\n",
      "Entity: ▁Al, Label: LABEL_0\n",
      "Entity: gor, Label: LABEL_0\n",
      "Entity: ith, Label: LABEL_0\n",
      "Entity: m, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Book, Label: LABEL_1\n",
      "Entity: ▁Review, Label: LABEL_1\n",
      "Entity: ▁Site, Label: LABEL_0\n",
      "Entity: ▁using, Label: LABEL_0\n",
      "Entity: ▁CSS, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_0\n",
      "Entity: ▁H, Label: LABEL_0\n",
      "Entity: t, Label: LABEL_1\n",
      "Entity: ml, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_0\n",
      "Entity: ▁Java, Label: LABEL_0\n",
      "Entity: script, Label: LABEL_0\n",
      "Entity: ,, Label: LABEL_0\n",
      "Entity: ▁PHP, Label: LABEL_0\n",
      "Entity: ▁, Label: LABEL_0\n",
      "Entity: <unk>, Label: LABEL_0\n",
      "Entity: ▁Railway, Label: LABEL_0\n",
      "Entity: ▁Reservation, Label: LABEL_0\n",
      "Entity: ▁System, Label: LABEL_0\n",
      "Entity: ▁using, Label: LABEL_0\n",
      "Entity: ▁Python, Label: LABEL_0\n",
      "Entity: ▁Fla, Label: LABEL_0\n",
      "Entity: sk, Label: LABEL_1\n"
     ]
    }
   ],
   "source": [
    "#new added code for NER\n",
    "import pandas as pd\n",
    "from transformers import XLNetTokenizer, XLNetForTokenClassification, pipeline\n",
    "\n",
    "# Load your labeled résumé dataset (assuming it's in CSV format)\n",
    "#resume_data = filteredtxt\n",
    "\n",
    "# Load the XLNet tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = XLNetForTokenClassification.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Create a NER pipeline\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # Perform NER\n",
    "ner_results = nlp_ner(Text)\n",
    "\n",
    "    # Print the results for this résumé\n",
    "#print(f\"Résumé {index + 1} NER Results:\")\n",
    "for result in ner_results:\n",
    "    print(f\"Entity: {result['word']}, Label: {result['entity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Avanti, Label: PERSON\n",
      "Entity: Makarand, Label: ORGANIZATION\n",
      "Entity: CAREER, Label: ORGANIZATION\n",
      "Entity: EDUCATION, Label: ORGANIZATION\n",
      "Entity: Mumbai, Label: GPE\n",
      "Entity: Mumbai, Label: PERSON\n",
      "Entity: Computer Science, Label: ORGANIZATION\n",
      "Entity: Data, Label: GPE\n",
      "Entity: Natural Language, Label: ORGANIZATION\n",
      "Entity: Pillai College, Label: PERSON\n",
      "Entity: Of Engineering, Label: ORGANIZATION\n",
      "Entity: Panvel, Label: PERSON\n",
      "Entity: Jindal Mount Litera Zee School, Label: PERSON\n",
      "Entity: Nagothane, Label: PERSON\n",
      "Entity: XII Science, Label: ORGANIZATION\n",
      "Entity: PCMB, Label: ORGANIZATION\n",
      "Entity: Jindal Vidya Mandir Salav, Label: PERSON\n",
      "Entity: Murud, Label: PERSON\n",
      "Entity: Python, Label: PERSON\n",
      "Entity: Books ACHIEVEMENTS, Label: PERSON\n",
      "Entity: Project, Label: GPE\n",
      "Entity: Leader, Label: ORGANIZATION\n",
      "Entity: PROJECTS, Label: ORGANIZATION\n",
      "Entity: Diabetes Detection System, Label: PERSON\n",
      "Entity: Python, Label: PERSON\n",
      "Entity: Flask Application, Label: PERSON\n",
      "Entity: SVM, Label: ORGANIZATION\n",
      "Entity: Support Vector Machine Algorithm, Label: PERSON\n",
      "Entity: CSS, Label: ORGANIZATION\n",
      "Entity: Html, Label: PERSON\n",
      "Entity: Javascript, Label: PERSON\n",
      "Entity: PHP, Label: ORGANIZATION\n",
      "Entity: Railway Reservation System, Label: PERSON\n",
      "Entity: Python Flask, Label: PERSON\n"
     ]
    }
   ],
   "source": [
    "#NER using nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Sample text\n",
    "#text = \"Barack Obama was born in Hawaii and works in Washington, D.C.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(Text)\n",
    "\n",
    "# Perform Part-of-Speech tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Perform NER using NLTK's ne_chunk\n",
    "ner_results = ne_chunk(pos_tags)\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = []\n",
    "for subtree in ner_results:\n",
    "    if type(subtree) == nltk.Tree:\n",
    "        entity_label = subtree.label()\n",
    "        entity_text = \" \".join([word for word, tag in subtree.leaves()])\n",
    "        named_entities.append((entity_text, entity_label))\n",
    "\n",
    "# Print the named entities\n",
    "for entity, label in named_entities:\n",
    "    print(f\"Entity: {entity}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "\n",
    "# Load your JSON dataset\n",
    "with open(\"traindata3.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    dataset = json.load(json_file)\n",
    "\n",
    "# Step 2: Tokenize and Preprocess the Dataset\n",
    "\n",
    "model_name = \"xlnet-base-cased\"\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.dataset[idx]['content']\n",
    "        label = self.dataset[idx]['label']  # Use 'label' as the key for the label field.\n",
    "\n",
    "        # Tokenize and preprocess the content\n",
    "        inputs = self.tokenizer(\n",
    "            content,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create instances of CustomDataset for training and validation\n",
    "train_dataset = CustomDataset(dataset, tokenizer)  # Assuming 'train' is the key for training data.\n",
    "#val_dataset = CustomDataset(dataset['val'], tokenizer)      # Assuming 'val' is the key for validation data.\n",
    "\n",
    "# Step 3: Fine-Tuning Configuration\n",
    "\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Step 4: Load Pretrained XLNet Model\n",
    "model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Change num_labels based on your task.\n",
    "\n",
    "# Step 5: Prepare Data Loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(batch\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[53], line 26\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     25\u001b[0m     content \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx][\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]  \u001b[39m# Use 'label' as the key for the label field.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39m# Tokenize and preprocess the content\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m     30\u001b[0m         content,\n\u001b[0;32m     31\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     35\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Govardhana K\\nSenior Software Engineer\\n\\nBengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n\\nTotal IT experience 5 Years 6 Months\\nCloud Lending Solutions INC 4 Month • Salesforce Developer\\nOracle 5 Years 2 Month • Core Java Developer\\nLanguages Core Java, Go Lang\\nOracle PL-SQL programming,\\nSales Force Developer with APEX.\\n\\nDesignations & Promotions\\n\\nWilling to relocate: Anywhere\\n\\nWORK EXPERIENCE\\n\\nSenior Software Engineer\\n\\nCloud Lending Solutions -  Bangalore, Karnataka -\\n\\nJanuary 2018 to Present\\n\\nPresent\\n\\nSenior Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2016 to December 2017\\n\\nStaff Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nJanuary 2014 to October 2016\\n\\nAssociate Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2012 to December 2013\\n\\nEDUCATION\\n\\nB.E in Computer Science Engineering\\n\\nAdithya Institute of Technology -  Tamil Nadu\\n\\nSeptember 2008 to June 2012\\n\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nSKILLS\\n\\nAPEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n\\nLINKS\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Proficiency:\\n\\nLanguages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/',\n",
       " 'annotation': [{'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1749, 'end': 1754, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1696, 'end': 1701, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1417, 'end': 1422, 'text': 'Oracle'}]},\n",
       "  {'label': ['Skills'],\n",
       "   'points': [{'start': 1356,\n",
       "     'end': 1792,\n",
       "     'text': 'Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1209, 'end': 1214, 'text': 'Oracle'}]},\n",
       "  {'label': ['Skills'],\n",
       "   'points': [{'start': 1136,\n",
       "     'end': 1247,\n",
       "     'text': 'APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n'}]},\n",
       "  {'label': ['Graduation Year'],\n",
       "   'points': [{'start': 928, 'end': 931, 'text': '2012'}]},\n",
       "  {'label': ['College Name'],\n",
       "   'points': [{'start': 858,\n",
       "     'end': 888,\n",
       "     'text': 'Adithya Institute of Technology'}]},\n",
       "  {'label': ['Degree'],\n",
       "   'points': [{'start': 821,\n",
       "     'end': 855,\n",
       "     'text': 'B.E in Computer Science Engineering'}]},\n",
       "  {'label': ['Graduation Year'],\n",
       "   'points': [{'start': 787, 'end': 790, 'text': '2012'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 744, 'end': 749, 'text': 'Oracle'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 722, 'end': 741, 'text': 'Associate Consultant'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 658, 'end': 663, 'text': 'Oracle'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 640, 'end': 655, 'text': 'Staff Consultant'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 574, 'end': 579, 'text': 'Oracle'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 555, 'end': 572, 'text': 'Senior Consultant\\n'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 470, 'end': 492, 'text': 'Cloud Lending Solutions'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 444,\n",
       "     'end': 468,\n",
       "     'text': 'Senior Software Engineer\\n'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 308, 'end': 313, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 234, 'end': 239, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 175, 'end': 197, 'text': 'Cloud Lending Solutions'}]},\n",
       "  {'label': ['Email Address'],\n",
       "   'points': [{'start': 93,\n",
       "     'end': 136,\n",
       "     'text': 'indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n'}]},\n",
       "  {'label': ['Location'],\n",
       "   'points': [{'start': 39, 'end': 47, 'text': 'Bengaluru'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 13, 'end': 37, 'text': 'Senior Software Engineer\\n'}]},\n",
       "  {'label': ['Name'],\n",
       "   'points': [{'start': 0, 'end': 11, 'text': 'Govardhana K'}]}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[0;32m      7\u001b[0m         inputs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[72], line 26\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     25\u001b[0m     content \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx][\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]  \u001b[39m# Use 'label' as the key for the label field.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39m# Tokenize and preprocess the content\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[0;32m     30\u001b[0m         content,\n\u001b[0;32m     31\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     35\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Step 6: Define Training Loop\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Step 7: Evaluate Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 8: Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"fine_tuned_xlnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input and output file paths\n",
    "input_file_path = \"traindata.json\"\n",
    "output_file_path = \"traindata3.json\"\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, \\\n",
    "     open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "\n",
    "    # Initialize a flag to track if it's the first line\n",
    "    first_line = True\n",
    "\n",
    "    # Iterate through the lines in the input file\n",
    "    for line in input_file:\n",
    "        # Add a comma before each line (except the first line)\n",
    "        if not first_line:\n",
    "            output_file.write(\",\")\n",
    "        else:\n",
    "            first_line = False\n",
    "\n",
    "        # Write the line from the input file to the output file\n",
    "        output_file.write(line)\n",
    "\n",
    "    # Write a closing square bracket to complete the JSON array\n",
    "    output_file.write(\"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile No.:  +918605131403\n"
     ]
    }
   ],
   "source": [
    "def extract_mobile_number(text):\n",
    "    '''\n",
    "    Helper function to extract mobile number from text\n",
    "\n",
    "    :param text: plain text extracted from resume file\n",
    "    :return: string of extracted mobile numbers\n",
    "    '''\n",
    "    # Found this complicated regex on : https://zapier.com/blog/extract-links-email-phone-regex/\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "print(\"Mobile No.: \",extract_mobile_number(filtered_data))\n",
    "resume_data[titles[2]]=extract_mobile_number(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['System', 'Design', 'Php', 'Css', 'C', 'Django', 'Html', 'Programming', 'Javascript', 'Engineering', 'Flask', 'Python', 'Java']\n"
     ]
    }
   ],
   "source": [
    "def list_to_string(lst):\n",
    "    # Convert the list to a string representation\n",
    "    return ', '.join(map(str, lst))\n",
    "noun_chunks=[]\n",
    "extracted_skills=[]\n",
    "file='skills.csv'\n",
    "new_nlp = nlp(filtered_data)\n",
    "def extract_skills(nlp_text, noun_chunks):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    data = pd.read_csv(os.path.join(os.path.dirname(file), 'skills.csv')) \n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams\n",
    "    for token in noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    extracted_skills=[i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "extract_skills(new_nlp,noun_chunks)\n",
    "extracted_skills=extract_skills(new_nlp,noun_chunks)\n",
    "print(extracted_skills)\n",
    "resume_data[titles[5]]=list_to_string(extracted_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def create_excel_sheet(data):\n",
    "    # Create a new Excel workbook\n",
    "    wb = Workbook()\n",
    "    \n",
    "    # Get the current timestamp for the sheet name\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    \n",
    "    # Create a new sheet with the timestamp as the name\n",
    "    sheet = wb.create_sheet(title=timestamp)\n",
    "    \n",
    "    # Create a DataFrame from the list\n",
    "    df = pd.DataFrame.from_dict(resume_data, orient='index', columns=['Text'])\n",
    "\n",
    "\n",
    "    \n",
    "    # Write the data to the Excel sheet\n",
    "    for index, row in df.iterrows():\n",
    "        sheet.append(row.tolist())\n",
    "    \n",
    "    # Save the Excel workbook\n",
    "    wb.save(f\"output_{timestamp}.xlsx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample list to convert\n",
    "    my_list = extract_skills(new_nlp,noun_chunks)\n",
    "    \n",
    "    # Convert the list to a single string\n",
    "    converted_data = list_to_string(my_list)\n",
    "    \n",
    "    # Create a new Excel sheet with the converted data\n",
    "    create_excel_sheet([converted_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def create_excel_sheet(dataframe):\n",
    "    # Create a new Excel workbook\n",
    "    wb = Workbook()\n",
    "    \n",
    "    # Get the current timestamp for the sheet name\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    \n",
    "    # Create a new sheet with the timestamp as the name\n",
    "    sheet = wb.create_sheet(title=timestamp)\n",
    "    \n",
    "    # Convert the DataFrame to a list of lists for easy insertion\n",
    "    data = dataframe.values.tolist()\n",
    "    \n",
    "    # Write the data to the Excel sheet\n",
    "    for row in data:\n",
    "        sheet.append(row)\n",
    "    \n",
    "    # Save the Excel workbook\n",
    "    wb.save(f\"output_{timestamp}.xlsx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample DataFrame\n",
    "    data = {'Column1': [1, 2, 3, 4],\n",
    "            'Column2': ['A', 'B', 'C', 'D']}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create a new Excel sheet with the DataFrame\n",
    "    create_excel_sheet(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System, Design, Php, Css, C, Django, Html, Programming, Javascript, Engineering, Flask, Python, Java\n"
     ]
    }
   ],
   "source": [
    "print(list_to_string(my_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching the extracted skills with our requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills in Resume:  ['System', 'Design', 'Php', 'Css', 'C', 'Django', 'Html', 'Programming', 'Javascript', 'Engineering', 'Flask', 'Python', 'Java']\n",
      "Required skills are:  ['engineering', 'html', 'c', 'java', 'javascript', 'php', 'python', 'machine learning', 'designing']\n",
      "Matched skills are:  ['Php', 'C', 'Html', 'Javascript', 'Engineering', 'Python', 'Java']\n",
      "The percentage of skills matched is:  77.77777777777779\n"
     ]
    }
   ],
   "source": [
    "file='required.csv'\n",
    "new_nlp = nlp(filtered_data)\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def match_skills(skills):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    # tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    # skills=[]\n",
    "    data = pd.read_csv(os.path.join(os.path.dirname(file), 'required.csv')) \n",
    "    required = list(data.columns.values)\n",
    "    print(\"Skills in Resume: \",skills)\n",
    "    print(\"Required skills are: \",required)\n",
    "    matched = []\n",
    "    # check for one-grams\n",
    "    for skill in skills:\n",
    "        if skill.lower() in required:\n",
    "            matched.append(skill)\n",
    "    print(\"Matched skills are: \",matched)\n",
    "    res = len(set(required) and set(matched)) / float(len(set(required) or set(matched))) * 100\n",
    "    print(\"The percentage of skills matched is: \",res)\n",
    "\n",
    "match_skills(extracted_skills)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education Qualification:  ['X']\n"
     ]
    }
   ],
   "source": [
    "def extract_education(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract education from spacy nlp text\n",
    "\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: tuple of education degree and year if year if found else only returns education degree\n",
    "    '''\n",
    "    EDUCATION         = [\n",
    "                    'BE','B.E.', 'B.E', 'BS', 'B.S', 'ME', 'M.E', 'M.E.', 'MS', 'M.S', 'BTECH', 'MTECH', \n",
    "                    'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "                    ]\n",
    "    \n",
    "    STOPWORDS         = set(stopwords.words('english'))\n",
    "    \n",
    "    YEAR              = r'(((20|19)(\\d{2})))'\n",
    "    \n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(YEAR), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year.group(0))))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "print(\"Education Qualification: \",extract_education(Text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_experience(resume_text):\n",
    "    '''\n",
    "    Helper function to extract experience from resume text\n",
    "\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: list of experience\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # word tokenization \n",
    "    word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "    # remove stop words and lemmatize  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and wordnet_lemmatizer.lemmatize(w) not in stop_words] \n",
    "    sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    # parse regex\n",
    "    cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "    cs = cp.parse(sent)\n",
    "    \n",
    "    # for i in cs.subtrees(filter=lambda x: x.label() == 'P'):\n",
    "    #     print(i)\n",
    "    \n",
    "    test = []\n",
    "    \n",
    "    for vp in list(cs.subtrees(filter=lambda x: x.label()=='P')):\n",
    "        test.append(\" \".join([i[0] for i in vp.leaves() if len(vp.leaves()) >= 2]))\n",
    "\n",
    "    # Search the word 'experience' in the chunk and then print out the text after it\n",
    "    x = [x[x.lower().index('experience') + 10:] for i, x in enumerate(test) if x and 'experience' in x.lower()]\n",
    "    return x\n",
    "print(extract_experience(Text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string_found' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 176\u001b[0m\n\u001b[0;32m    174\u001b[0m                     competency_dict[competency]\u001b[39m.\u001b[39mappend(item)\n\u001b[0;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m competency_dict\n\u001b[1;32m--> 176\u001b[0m \u001b[39mprint\u001b[39m(extract_competencies(Text, exp_list))    \n",
      "Cell \u001b[1;32mIn[63], line 170\u001b[0m, in \u001b[0;36mextract_competencies\u001b[1;34m(text, experience_list)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mfor\u001b[39;00m competency \u001b[39min\u001b[39;00m COMPETENCIES\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m COMPETENCIES[competency]:\n\u001b[1;32m--> 170\u001b[0m         \u001b[39mif\u001b[39;00m string_found(item, experience_text):\n\u001b[0;32m    171\u001b[0m             \u001b[39mif\u001b[39;00m competency \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m competency_dict\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    172\u001b[0m                 competency_dict[competency] \u001b[39m=\u001b[39m [item]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string_found' is not defined"
     ]
    }
   ],
   "source": [
    "exp_list=[]\n",
    "\n",
    "def extract_competencies(text, experience_list):\n",
    "    '''\n",
    "    Helper function to extract competencies from resume text\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: dictionary of competencies\n",
    "    '''\n",
    "    experience_text = ' '.join(experience_list)\n",
    "    competency_dict = {}\n",
    "    COMPETENCIES = {\n",
    "    'teamwork': [\n",
    "        'supervised',\n",
    "        'facilitated',\n",
    "        'planned',\n",
    "        'plan',\n",
    "        'served',\n",
    "        'serve',\n",
    "        'project lead',\n",
    "        'managing',\n",
    "        'managed',\n",
    "        'lead ',\n",
    "        'project team',\n",
    "        'team',\n",
    "        'conducted',\n",
    "        'worked',\n",
    "        'gathered',\n",
    "        'organized',\n",
    "        'mentored',\n",
    "        'assist',\n",
    "        'review',\n",
    "        'help',\n",
    "        'involve',\n",
    "        'share',\n",
    "        'support',\n",
    "        'coordinate',\n",
    "        'cooperate',\n",
    "        'contributed'\n",
    "    ],\n",
    "    'communication': [\n",
    "        'addressed',\n",
    "        'collaborated',\n",
    "        'conveyed',\n",
    "        'enlivened',\n",
    "        'instructed',\n",
    "        'performed',\n",
    "        'presented',\n",
    "        'spoke',\n",
    "        'trained',\n",
    "        'author',\n",
    "        'communicate',\n",
    "        'define',\n",
    "        'influence',\n",
    "        'negotiated',\n",
    "        'outline',\n",
    "        'proposed',\n",
    "        'persuaded',\n",
    "        'edit',\n",
    "        'interviewed',\n",
    "        'summarize',\n",
    "        'translate',\n",
    "        'write',\n",
    "        'wrote',\n",
    "        'project plan',\n",
    "        'business case',\n",
    "        'proposal',\n",
    "        'writeup'\n",
    "    ],\n",
    "    'analytical': [\n",
    "        'process improvement',\n",
    "        'competitive analysis',\n",
    "        'aligned',\n",
    "        'strategive planning',\n",
    "        'cost savings',\n",
    "        'researched ',\n",
    "        'identified',\n",
    "        'created',\n",
    "        'led',\n",
    "        'measure',\n",
    "        'program',\n",
    "        'quantify',\n",
    "        'forecasr',\n",
    "        'estimate',\n",
    "        'analyzed',\n",
    "        'survey',\n",
    "        'reduced',\n",
    "        'cut cost',\n",
    "        'conserved',\n",
    "        'budget',\n",
    "        'balanced',\n",
    "        'allocate',\n",
    "        'adjust',\n",
    "        'lauched',\n",
    "        'hired',\n",
    "        'spedup',\n",
    "        'speedup',\n",
    "        'ran',\n",
    "        'run',\n",
    "        'enchanced',\n",
    "        'developed'\n",
    "    ],\n",
    "    'result_driven': [\n",
    "        'cut',\n",
    "        'decrease',\n",
    "        'eliminate',\n",
    "        'increase',\n",
    "        'lower',\n",
    "        'maximize',\n",
    "        'rasie',\n",
    "        'reduce',\n",
    "        'accelerate',\n",
    "        'accomplish',\n",
    "        'advance',\n",
    "        'boost',\n",
    "        'change',\n",
    "        'improve',\n",
    "        'saved',\n",
    "        'save',\n",
    "        'solve',\n",
    "        'solved',\n",
    "        'upgrade',\n",
    "        'fix',\n",
    "        'fixed',\n",
    "        'correct',\n",
    "        'achieve'           \n",
    "    ],\n",
    "    'leadership': [\n",
    "        'advise',\n",
    "        'coach',\n",
    "        'guide',\n",
    "        'influence',\n",
    "        'inspire',\n",
    "        'instruct',\n",
    "        'teach',\n",
    "        'authorized',\n",
    "        'chair',\n",
    "        'control',\n",
    "        'establish',\n",
    "        'execute',\n",
    "        'hire',\n",
    "        'multi-task',\n",
    "        'oversee',\n",
    "        'navigate',\n",
    "        'prioritize',\n",
    "        'approve',\n",
    "        'administer',\n",
    "        'preside',\n",
    "        'enforce',\n",
    "        'delegate',\n",
    "        'coordinate',\n",
    "        'streamlined',\n",
    "        'produce',\n",
    "        'review',\n",
    "        'supervise',\n",
    "        'terminate',\n",
    "        'found',\n",
    "        'set up',\n",
    "        'spearhead',\n",
    "        'originate',\n",
    "        'innovate',\n",
    "        'implement',\n",
    "        'design',\n",
    "        'launch',\n",
    "        'pioneer',\n",
    "        'institute'\n",
    "    ]\n",
    "}\n",
    "    for competency in COMPETENCIES.keys():\n",
    "        for item in COMPETENCIES[competency]:\n",
    "            if string_found(item, experience_text):\n",
    "                if competency not in competency_dict.keys():\n",
    "                    competency_dict[competency] = [item]\n",
    "                else:\n",
    "                    competency_dict[competency].append(item)\n",
    "    return competency_dict\n",
    "print(extract_competencies(Text, exp_list))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_measurable_results(text, experience_list):\n",
    "    '''\n",
    "    Helper function to extract measurable results from resume text\n",
    "\n",
    "    :param resume_text: Plain resume text\n",
    "    :return: dictionary of measurable results\n",
    "    '''\n",
    "\n",
    "    # we scan for measurable results only in first half of each sentence\n",
    "    experience_text = ' '.join([text[:len(text) // 2 - 1] for text in experience_list])\n",
    "    mr_dict = {}\n",
    "\n",
    "    for mr in cs.MEASURABLE_RESULTS.keys():\n",
    "        for item in cs.MEASURABLE_RESULTS[mr]:\n",
    "            if string_found(item, experience_text):\n",
    "                if mr not in mr_dict.keys():\n",
    "                    mr_dict[mr] = [item]\n",
    "                else:\n",
    "                    mr_dict[mr].append(item)\n",
    "    \n",
    "    return mr_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ner using xlnet testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "\n",
    "# Load your JSON dataset\n",
    "with open(\"traindata3.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Govardhana K\\nSenior Software Engineer\\n\\nBengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n\\nTotal IT experience 5 Years 6 Months\\nCloud Lending Solutions INC 4 Month • Salesforce Developer\\nOracle 5 Years 2 Month • Core Java Developer\\nLanguages Core Java, Go Lang\\nOracle PL-SQL programming,\\nSales Force Developer with APEX.\\n\\nDesignations & Promotions\\n\\nWilling to relocate: Anywhere\\n\\nWORK EXPERIENCE\\n\\nSenior Software Engineer\\n\\nCloud Lending Solutions -  Bangalore, Karnataka -\\n\\nJanuary 2018 to Present\\n\\nPresent\\n\\nSenior Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2016 to December 2017\\n\\nStaff Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nJanuary 2014 to October 2016\\n\\nAssociate Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2012 to December 2013\\n\\nEDUCATION\\n\\nB.E in Computer Science Engineering\\n\\nAdithya Institute of Technology -  Tamil Nadu\\n\\nSeptember 2008 to June 2012\\n\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nSKILLS\\n\\nAPEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n\\nLINKS\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Proficiency:\\n\\nLanguages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/',\n",
       " 'annotation': [{'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1749, 'end': 1754, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1696, 'end': 1701, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1417, 'end': 1422, 'text': 'Oracle'}]},\n",
       "  {'label': ['Skills'],\n",
       "   'points': [{'start': 1356,\n",
       "     'end': 1792,\n",
       "     'text': 'Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 1209, 'end': 1214, 'text': 'Oracle'}]},\n",
       "  {'label': ['Skills'],\n",
       "   'points': [{'start': 1136,\n",
       "     'end': 1247,\n",
       "     'text': 'APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n'}]},\n",
       "  {'label': ['Graduation Year'],\n",
       "   'points': [{'start': 928, 'end': 931, 'text': '2012'}]},\n",
       "  {'label': ['College Name'],\n",
       "   'points': [{'start': 858,\n",
       "     'end': 888,\n",
       "     'text': 'Adithya Institute of Technology'}]},\n",
       "  {'label': ['Degree'],\n",
       "   'points': [{'start': 821,\n",
       "     'end': 855,\n",
       "     'text': 'B.E in Computer Science Engineering'}]},\n",
       "  {'label': ['Graduation Year'],\n",
       "   'points': [{'start': 787, 'end': 790, 'text': '2012'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 744, 'end': 749, 'text': 'Oracle'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 722, 'end': 741, 'text': 'Associate Consultant'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 658, 'end': 663, 'text': 'Oracle'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 640, 'end': 655, 'text': 'Staff Consultant'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 574, 'end': 579, 'text': 'Oracle'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 555, 'end': 572, 'text': 'Senior Consultant\\n'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 470, 'end': 492, 'text': 'Cloud Lending Solutions'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 444,\n",
       "     'end': 468,\n",
       "     'text': 'Senior Software Engineer\\n'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 308, 'end': 313, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 234, 'end': 239, 'text': 'Oracle'}]},\n",
       "  {'label': ['Companies worked at'],\n",
       "   'points': [{'start': 175, 'end': 197, 'text': 'Cloud Lending Solutions'}]},\n",
       "  {'label': ['Email Address'],\n",
       "   'points': [{'start': 93,\n",
       "     'end': 136,\n",
       "     'text': 'indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n'}]},\n",
       "  {'label': ['Location'],\n",
       "   'points': [{'start': 39, 'end': 47, 'text': 'Bengaluru'}]},\n",
       "  {'label': ['Designation'],\n",
       "   'points': [{'start': 13, 'end': 37, 'text': 'Senior Software Engineer\\n'}]},\n",
       "  {'label': ['Name'],\n",
       "   'points': [{'start': 0, 'end': 11, 'text': 'Govardhana K'}]}]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label_and_points(label, points):\n",
    "    # Initialize an empty list to store the extracted entity texts\n",
    "    entity_texts = []\n",
    "    \n",
    "    # Loop through each point dictionary in the list\n",
    "    for point in points:\n",
    "        start = point['start']\n",
    "        end = point['end']\n",
    "        \n",
    "        # Extract the labeled entity text from the content\n",
    "        entity_text = content[start:end]\n",
    "        \n",
    "        # Append the entity text to the list\n",
    "        entity_texts.append(entity_text)\n",
    "    \n",
    "    # Combine the extracted entity texts if needed (e.g., join with a space)\n",
    "    combined_entity_text = ' '.join(entity_texts)\n",
    "    \n",
    "    return combined_entity_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the XLNet tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Tokenize and pad/truncate input sequences\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in Text:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,  # Adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_text['input_ids'])\n",
    "    attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "# Convert input_ids and attention_masks to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Encode the target labels (NER tags) into tensors or other suitable format\n",
    "# You may need to use label encoders or other methods based on your model's requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   5,    5,    5,  ...,   79,    4,    3],\n",
       "        [   5,    5,    5,  ..., 2721,    4,    3],\n",
       "        [   5,    5,    5,  ...,   24,    4,    3],\n",
       "        ...,\n",
       "        [   5,    5,    5,  ...,    5,    4,    3],\n",
       "        [   5,    5,    5,  ...,    5,    4,    3],\n",
       "        [   5,    5,    5,  ...,    5,    4,    3]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, target_labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (list of torch.Tensor): List of input sequences as tensors.\n",
    "            attention_masks (list of torch.Tensor): List of attention masks as tensors.\n",
    "            target_labels (list of list of str): List of target labels for NER, where each list contains labels for one example.\n",
    "        \"\"\"\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.target_labels = target_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'target_labels': self.target_labels[idx],\n",
    "        }\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/222 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 29\u001b[0m\n\u001b[0;32m     22\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     23\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     24\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     25\u001b[0m     train_dataset\u001b[39m=\u001b[39mNERDataset(input_ids, attention_masks, target_labels),  \u001b[39m# Define your custom dataset class\u001b[39;00m\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1815\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1816\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[81], line 23\u001b[0m, in \u001b[0;36mNERDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     20\u001b[0m     sample \u001b[39m=\u001b[39m {\n\u001b[0;32m     21\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[idx],\n\u001b[0;32m     22\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_masks[idx],\n\u001b[1;32m---> 23\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtarget_labels\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_labels[idx],\n\u001b[0;32m     24\u001b[0m     }\n\u001b[0;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m sample\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetForTokenClassification, Trainer, TrainingArguments\n",
    "label_list = ['College Name', 'Designation', 'Name', 'Companies worked at', 'Email Address', 'Location', 'Skills']\n",
    "target_labels = [\n",
    "    [\"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", ],  # Labels for the first example\n",
    "    [\"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", ]  # Labels for the second example\n",
    "    # Add labels for more examples...\n",
    "]\n",
    "\n",
    "# Calculate the number of unique labels\n",
    "num_labels = len(label_list)# Load the pre-trained XLNet model for token classification\n",
    "model = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', num_labels=num_labels)  # Define num_labels accordingly\n",
    "\n",
    "# Define training arguments (batch size, learning rate, etc.)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./ner_model',\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=NERDataset(input_ids, attention_masks, target_labels),  # Define your custom dataset class\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.32.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.0.0)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Obtaining dependency information for accelerate>=0.20.3 from https://files.pythonhosted.org/packages/4d/a7/05c67003d659a0035f2b3a8cf389c1d9645865aee84a73ce99ddab16682f/accelerate-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\mrunal kulkarni\\appdata\\roaming\\python\\python311\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "   ---------------------------------------- 0.0/251.2 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 92.2/251.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 225.3/251.2 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 251.2/251.2 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.22.0\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mrunal kulkarni\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mrunal kulkarni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
