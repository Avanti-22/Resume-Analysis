{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# importing all the libraries\n",
                "import io\n",
                "import os\n",
                "import re\n",
                "import pandas as pd\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WordPunctTokenizer, MWETokenizer\n",
                "# import PyPDF2\n",
                "# for pdf to txt\n",
                "import pdfminer\n",
                "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
                "from pdfminer.layout import LAParams\n",
                "from pdfminer.converter import TextConverter\n",
                "from io import StringIO\n",
                "from pdfminer.pdfpage import PDFPage\n",
                "import openpyxl as px\n",
                "import docx2txt\n",
                "import constants as cs\n",
                "import string\n",
                "# import utils\n",
                "import pprint\n",
                "from spacy.matcher import matcher\n",
                "import multiprocessing as mp\n",
                "import warnings\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "# from constants import STOPWORDS\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "titles = [\"Name\", \"Email\", \"Phone\", \"Education\", \"Experience\", \"Skills\"]\n",
                "resume_data={}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resume/Mrunalresume.pdf\n"
                    ]
                }
            ],
            "source": [
                "resume_file=input('Enter resume: ')\n",
                "print(resume_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resume/Mrunalresume.pdf\n"
                    ]
                }
            ],
            "source": [
                "print(resume_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract txt from pdf\n",
                "# pdf_path=\"code\\Resume.pdf\"\n",
                "def pdf_to_txt(pdf_path):\n",
                "    resource_manager = PDFResourceManager(caching=True)\n",
                "    \n",
                "    # create a string object that will contain the final text the representation of the pdf. \n",
                "    out_text = StringIO()\n",
                "    codec = 'utf-8'\n",
                "    laParams = LAParams()\n",
                "    \n",
                "    # Create a TextConverter Object:\n",
                "    text_converter = TextConverter(resource_manager, out_text, laparams=laParams)\n",
                "    fp = open(pdf_path, 'rb')\n",
                "    #Create a PDF interpreter object\n",
                "\n",
                "    interpreter = PDFPageInterpreter(resource_manager, text_converter)\n",
                "    \n",
                "    # We are going to process the content of each page of the original PDF File    \n",
                "    for page in PDFPage.get_pages(fp, pagenos=set(), maxpages=0, password=\"\", caching=True, check_extractable=True):\n",
                "        interpreter.process_page(page)\n",
                "\n",
                "    \n",
                "    # Retrieve the entire contents of the “file” at any time \n",
                "    text = out_text.getvalue()\n",
                "\n",
                "    # Closing all the ressources we previously opened\n",
                "\n",
                "    fp.close()\n",
                "    text_converter.close()\n",
                "    out_text.close()\n",
                "    \n",
                "    return text\n",
                "# retext = pdf_to_txt(pdf_path)\n",
                "# print(pdf_to_txt(pdf_path))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract txt from\n",
                "# docx_path=\"Resume.docx\"\n",
                "def docx_to_txt(docx_path):\n",
                "    temp = docx2txt.process(docx_path)\n",
                "    text = [line.replace('\t', ' ') for line in temp.split('') if line]\n",
                "    return ' '.join(text)\n",
                "# print(docx_to_txt(docx_path))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "('Resume/Mrunalresume', '.pdf')\n",
                        "File Name:  Resume/Mrunalresume\n",
                        "File Extension:  .pdf\n"
                    ]
                }
            ],
            "source": [
                "split_tup = os.path.splitext(resume_file)\n",
                "print(split_tup)\n",
                "  \n",
                "# extract the file name and extension\n",
                "file_name = split_tup[0]\n",
                "file_extension = split_tup[1]\n",
                "  \n",
                "print(\"File Name: \", file_name)\n",
                "print(\"File Extension: \", file_extension)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mrunal Kulkarni\n",
                        "Navi Mumbai\n",
                        "9423627124 | maheshkulkarni01121@gmail.com\n",
                        "LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni\n",
                        "\n",
                        "Career Objective\n",
                        "To learn and expand my knowledge in the field of computer science, particularly in the area of\n",
                        "data science through which I can earn a job which enables me to use my passion for A.I to grow\n",
                        "the company as well as accomplish my self-development goals.\n",
                        "\n",
                        "Education Qualification\n",
                        "\n",
                        "● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with\n",
                        "\n",
                        "aggregate of 9.37.\n",
                        "\n",
                        "Sr.no\n",
                        "\n",
                        "Qualification\n",
                        "\n",
                        "1\n",
                        "\n",
                        "2\n",
                        "\n",
                        "3\n",
                        "\n",
                        "B.tech\n",
                        "\n",
                        "HSC\n",
                        "\n",
                        "SSC\n",
                        "\n",
                        "9.58\n",
                        "\n",
                        "9.70\n",
                        "\n",
                        "9.48\n",
                        "\n",
                        "8.90\n",
                        "\n",
                        "9.14\n",
                        "\n",
                        "9.45\n",
                        "\n",
                        "Grade\n",
                        "\n",
                        "Sem 1\n",
                        "\n",
                        "Sem 2\n",
                        "\n",
                        "Sem 3\n",
                        "\n",
                        "Sem 4\n",
                        "\n",
                        "Sem 5\n",
                        "\n",
                        "Sem 6\n",
                        "\n",
                        "88.2%\n",
                        "\n",
                        "94.4%\n",
                        "\n",
                        "University/board\n",
                        "\n",
                        "Mumbai University\n",
                        "\n",
                        "CBSE\n",
                        "\n",
                        "CBSE\n",
                        "\n",
                        "Technical Skills\n",
                        "\n",
                        "● Python programming skills\n",
                        "● Machine learning\n",
                        "● Natural Language Processing(BERT , Pegasus)\n",
                        "● C programming skills\n",
                        "● Web development (HTML,CSS,PHP)\n",
                        "\n",
                        "\fProjects Done\n",
                        "\n",
                        "● Teams meeting summarization website using Pegasus model of Hugging Face\n",
                        "● Sentiment analysis using BERT\n",
                        "● Book recommendation website using Python,HTML\n",
                        "● Online Book review website\n",
                        "● Log book report generation website using PHP\n",
                        "\n",
                        "Courses Done\n",
                        "\n",
                        "● Fine tuning BERT for text classification using Python Tensorflow by Coursera\n",
                        "● Demystifying Networking course by IIT Bombay NPTEL\n",
                        "● Python Course by Kaggle\n",
                        "● C programming certificate by Spoken Tutorial\n",
                        "\n",
                        "Internships\n",
                        "\n",
                        "● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built\n",
                        "a log book report generation website using PHP , Codeigniter using the existing database\n",
                        "\n",
                        "Achievements\n",
                        "\n",
                        "● Reached Semi-finals of Deep Blue Season 8\n",
                        "\n",
                        "Extra Curriculars\n",
                        "\n",
                        "● Content writing awards in College\n",
                        "● Video Editing for social media\n",
                        "\n",
                        "Hobby\n",
                        "\n",
                        "● Story writing\n",
                        "● Reading\n",
                        "● Video editing\n",
                        "\n",
                        "Personal Details\n",
                        "\n",
                        "Name: Mrunal Mahesh Kulkarni\n",
                        "Address: BARC colony ,Tarpur\n",
                        "Phone No: 9423627124\n",
                        "Languages known: English, Hindi, Marathi\n",
                        "\n",
                        "Declaration: I hereby declare that the above information is true to my knowledge.\n",
                        "\n",
                        "Date: 3/8/23\n",
                        "Place: New Panvel\n",
                        "\n",
                        "\f\n"
                    ]
                }
            ],
            "source": [
                "# detect file extension and call above functions accordingly\n",
                "def extract_text(file_path, extension):\n",
                "    '''\n",
                "    Wrapper function to detect the file extension and call text extraction function accordingly\n",
                "\n",
                "    :param file_path: path of file of which text is to be extracted\n",
                "    :param extension: extension of file `file_name`\n",
                "    '''\n",
                "    text = ''\n",
                "    if extension == '.pdf':\n",
                "        for page in pdf_to_txt(file_path):\n",
                "            text += '' + page\n",
                "    elif extension == '.docx' or extension == '.doc':\n",
                "        text = docx_to_txt(file_path)\n",
                "    return text\n",
                "Text= extract_text(resume_file,file_extension)\n",
                "print(Text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tokenization Of the Extracted Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading collection 'all'\n",
                        "[nltk_data]    | \n",
                        "[nltk_data]    | Downloading package abc to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package abc is already up-to-date!\n",
                        "[nltk_data]    | Downloading package alpino to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package alpino is already up-to-date!\n",
                        "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
                        "[nltk_data]    |       to-date!\n",
                        "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
                        "[nltk_data]    |       up-to-date!\n",
                        "[nltk_data]    | Downloading package basque_grammars to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
                        "[nltk_data]    | Downloading package bcp47 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
                        "[nltk_data]    | Downloading package biocreative_ppi to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
                        "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
                        "[nltk_data]    | Downloading package book_grammars to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
                        "[nltk_data]    | Downloading package brown to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
                        "[nltk_data]    | Downloading package brown_tei to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
                        "[nltk_data]    | Downloading package cess_cat to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
                        "[nltk_data]    | Downloading package cess_esp to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
                        "[nltk_data]    | Downloading package chat80 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
                        "[nltk_data]    | Downloading package city_database to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
                        "[nltk_data]    | Downloading package cmudict to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
                        "[nltk_data]    | Downloading package comparative_sentences to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
                        "[nltk_data]    | Downloading package comtrans to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package conll2000 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
                        "[nltk_data]    | Downloading package conll2002 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
                        "[nltk_data]    | Downloading package conll2007 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package crubadan to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
                        "[nltk_data]    | Downloading package dependency_treebank to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
                        "[nltk_data]    | Downloading package dolch to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
                        "[nltk_data]    | Downloading package europarl_raw to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
                        "[nltk_data]    | Downloading package extended_omw to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package floresta to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
                        "[nltk_data]    | Downloading package framenet_v15 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
                        "[nltk_data]    | Downloading package framenet_v17 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
                        "[nltk_data]    | Downloading package gazetteers to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
                        "[nltk_data]    | Downloading package genesis to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
                        "[nltk_data]    | Downloading package gutenberg to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
                        "[nltk_data]    | Downloading package ieer to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
                        "[nltk_data]    | Downloading package inaugural to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
                        "[nltk_data]    | Downloading package indian to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
                        "[nltk_data]    | Downloading package jeita to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package kimmo to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
                        "[nltk_data]    | Downloading package knbc to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package large_grammars to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
                        "[nltk_data]    | Downloading package lin_thesaurus to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
                        "[nltk_data]    | Downloading package mac_morpho to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
                        "[nltk_data]    | Downloading package machado to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package masc_tagged to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
                        "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
                        "[nltk_data]    | Downloading package moses_sample to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
                        "[nltk_data]    | Downloading package movie_reviews to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
                        "[nltk_data]    | Downloading package mte_teip5 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
                        "[nltk_data]    | Downloading package mwa_ppdb to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
                        "[nltk_data]    | Downloading package names to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
                        "[nltk_data]    | Downloading package nombank.1.0 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
                        "[nltk_data]    | Downloading package nps_chat to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
                        "[nltk_data]    | Downloading package omw to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package omw-1.4 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package opinion_lexicon to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
                        "[nltk_data]    | Downloading package panlex_swadesh to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package paradigms to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
                        "[nltk_data]    | Downloading package pe08 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
                        "[nltk_data]    | Downloading package perluniprops to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
                        "[nltk_data]    | Downloading package pil to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
                        "[nltk_data]    | Downloading package pl196x to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
                        "[nltk_data]    | Downloading package porter_test to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
                        "[nltk_data]    | Downloading package ppattach to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
                        "[nltk_data]    | Downloading package problem_reports to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
                        "[nltk_data]    | Downloading package product_reviews_1 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
                        "[nltk_data]    | Downloading package product_reviews_2 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
                        "[nltk_data]    | Downloading package propbank to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package pros_cons to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
                        "[nltk_data]    | Downloading package ptb to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
                        "[nltk_data]    | Downloading package punkt to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
                        "[nltk_data]    | Downloading package qc to C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
                        "[nltk_data]    | Downloading package reuters to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package rslp to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
                        "[nltk_data]    | Downloading package rte to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
                        "[nltk_data]    | Downloading package sample_grammars to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
                        "[nltk_data]    | Downloading package semcor to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package senseval to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
                        "[nltk_data]    | Downloading package sentence_polarity to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
                        "[nltk_data]    | Downloading package sentiwordnet to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
                        "[nltk_data]    | Downloading package shakespeare to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
                        "[nltk_data]    | Downloading package sinica_treebank to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
                        "[nltk_data]    | Downloading package smultron to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
                        "[nltk_data]    | Downloading package snowball_data to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package spanish_grammars to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
                        "[nltk_data]    | Downloading package state_union to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
                        "[nltk_data]    | Downloading package stopwords to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Package stopwords is already up-to-date!\n",
                        "[nltk_data]    | Downloading package subjectivity to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
                        "[nltk_data]    | Downloading package swadesh to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
                        "[nltk_data]    | Downloading package switchboard to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
                        "[nltk_data]    | Downloading package tagsets to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
                        "[nltk_data]    | Downloading package timit to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
                        "[nltk_data]    | Downloading package toolbox to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
                        "[nltk_data]    | Downloading package treebank to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
                        "[nltk_data]    | Downloading package twitter_samples to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
                        "[nltk_data]    | Downloading package udhr to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
                        "[nltk_data]    | Downloading package udhr2 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
                        "[nltk_data]    | Downloading package unicode_samples to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
                        "[nltk_data]    | Downloading package universal_tagset to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
                        "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package vader_lexicon to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package verbnet to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
                        "[nltk_data]    | Downloading package verbnet3 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
                        "[nltk_data]    | Downloading package webtext to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
                        "[nltk_data]    | Downloading package wmt15_eval to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
                        "[nltk_data]    | Downloading package word2vec_sample to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
                        "[nltk_data]    | Downloading package wordnet to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package wordnet2021 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package wordnet2022 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
                        "[nltk_data]    | Downloading package wordnet31 to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    | Downloading package wordnet_ic to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
                        "[nltk_data]    | Downloading package words to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
                        "[nltk_data]    | Downloading package ycoe to\n",
                        "[nltk_data]    |     C:\\Users\\thale/nltk_data...\n",
                        "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
                        "[nltk_data]    | \n",
                        "[nltk_data]  Done downloading collection all\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "nltk.download('all')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Word Tokenize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "word_tokenize ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121', '@', 'gmail.com', 'LinkedIn', ':', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science', ',', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A.I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self-development', 'goals', '.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9.37', '.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2', '%', '94.4', '%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing', '(', 'BERT', ',', 'Pegasus', ')', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(', 'HTML', ',', 'CSS', ',', 'PHP', ')', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python', ',', 'HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center', ',', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name', ':', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address', ':', 'BARC', 'colony', ',', 'Tarpur', 'Phone', 'No', ':', '9423627124', 'Languages', 'known', ':', 'English', ',', 'Hindi', ',', 'Marathi', 'Declaration', ':', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge', '.', 'Date', ':', '3/8/23', 'Place', ':', 'New', 'Panvel']\n",
                        "TreebankWordTokenizer ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121', '@', 'gmail.com', 'LinkedIn', ':', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science', ',', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A.I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self-development', 'goals.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9.37.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2', '%', '94.4', '%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing', '(', 'BERT', ',', 'Pegasus', ')', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(', 'HTML', ',', 'CSS', ',', 'PHP', ')', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python', ',', 'HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center', ',', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name', ':', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address', ':', 'BARC', 'colony', ',', 'Tarpur', 'Phone', 'No', ':', '9423627124', 'Languages', 'known', ':', 'English', ',', 'Hindi', ',', 'Marathi', 'Declaration', ':', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge.', 'Date', ':', '3/8/23', 'Place', ':', 'New', 'Panvel']\n",
                        "WordPunctTokenizer ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121', '@', 'gmail', '.', 'com', 'LinkedIn', ':', 'www', '.', 'linkedin', '.', 'com', '/', 'in', '/', 'mrunal', '-', 'mahesh', '-', 'kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science', ',', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A', '.', 'I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self', '-', 'development', 'goals', '.', 'Education', 'Qualification', '●', 'Pursuing', 'B', '.', 'Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9', '.', '37', '.', 'Sr', '.', 'no', 'Qualification', '1', '2', '3', 'B', '.', 'tech', 'HSC', 'SSC', '9', '.', '58', '9', '.', '70', '9', '.', '48', '8', '.', '90', '9', '.', '14', '9', '.', '45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88', '.', '2', '%', '94', '.', '4', '%', 'University', '/', 'board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing', '(', 'BERT', ',', 'Pegasus', ')', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(', 'HTML', ',', 'CSS', ',', 'PHP', ')', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python', ',', 'HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center', ',', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi', '-', 'finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name', ':', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address', ':', 'BARC', 'colony', ',', 'Tarpur', 'Phone', 'No', ':', '9423627124', 'Languages', 'known', ':', 'English', ',', 'Hindi', ',', 'Marathi', 'Declaration', ':', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge', '.', 'Date', ':', '3', '/', '8', '/', '23', 'Place', ':', 'New', 'Panvel']\n",
                        "MWETokenizer ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121@gmail.com', 'LinkedIn:', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'and', 'expand', 'my', 'knowledge', 'in', 'the', 'field', 'of', 'computer', 'science,', 'particularly', 'in', 'the', 'area', 'of', 'data', 'science', 'through', 'which', 'I', 'can', 'earn', 'a', 'job', 'which', 'enables', 'me', 'to', 'use', 'my', 'passion', 'for', 'A.I', 'to', 'grow', 'the', 'company', 'as', 'well', 'as', 'accomplish', 'my', 'self-development', 'goals.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'in', 'computer', 'science', 'in', 'Pillai', 'College', 'of', 'Engineering', 'with', 'aggregate', 'of', '9.37.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2%', '94.4%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing(BERT', ',', 'Pegasus)', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(HTML,CSS,PHP)', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'of', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python,HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'for', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'by', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'by', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'by', 'Kaggle', '●', 'C', 'programming', 'certificate', 'by', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'at', 'Bhabha', 'Atomic', 'Research', 'Center,', 'Tarapur', 'where', 'I', 'built', 'a', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'the', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'of', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'in', 'College', '●', 'Video', 'Editing', 'for', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name:', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address:', 'BARC', 'colony', ',Tarpur', 'Phone', 'No:', '9423627124', 'Languages', 'known:', 'English,', 'Hindi,', 'Marathi', 'Declaration:', 'I', 'hereby', 'declare', 'that', 'the', 'above', 'information', 'is', 'true', 'to', 'my', 'knowledge.', 'Date:', '3/8/23', 'Place:', 'New', 'Panvel']\n",
                        "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni Career Objective To learn and expand my knowledge in the field of computer science, particularly in the area of data science through which I can earn a job which enables me to use my passion for A.I to grow the company as well as accomplish my self-development goals. Education Qualification ● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with aggregate of 9.37. Sr.no Qualification 1 2 3 B.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2% 94.4% University/board Mumbai University CBSE CBSE Technical Skills ● Python programming skills ● Machine learning ● Natural Language Processing(BERT , Pegasus) ● C programming skills ● Web development (HTML,CSS,PHP) Projects Done ● Teams meeting summarization website using Pegasus model of Hugging Face ● Sentiment analysis using BERT ● Book recommendation website using Python,HTML ● Online Book review website ● Log book report generation website using PHP Courses Done ● Fine tuning BERT for text classification using Python Tensorflow by Coursera ● Demystifying Networking course by IIT Bombay NPTEL ● Python Course by Kaggle ● C programming certificate by Spoken Tutorial Internships ● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built a log book report generation website using PHP , Codeigniter using the existing database Achievements ● Reached Semi-finals of Deep Blue Season 8 Extra Curriculars ● Content writing awards in College ● Video Editing for social media Hobby ● Story writing ● Reading ● Video editing Personal Details Name: Mrunal Mahesh Kulkarni Address: BARC colony ,Tarpur Phone No: 9423627124 Languages known: English, Hindi, Marathi Declaration: I hereby declare that the above information is true to my knowledge. Date: 3/8/23 Place: New Panvel\n"
                    ]
                }
            ],
            "source": [
                "#word tokenize\n",
                "print(\"word_tokenize\",word_tokenize(Text))\n",
                "\n",
                "tokenizer = TreebankWordTokenizer()\n",
                "print(\"TreebankWordTokenizer\",tokenizer.tokenize(Text))\n",
                "\n",
                "tokenizer = WordPunctTokenizer()\n",
                "print(\"WordPunctTokenizer\",tokenizer.tokenize(Text) )\n",
                "\n",
                "from nltk.tokenize import MWETokenizer\n",
                "mtokenizer = MWETokenizer([('athale20comp@studen', 't.mes.ac.in'), ('+91', '-', '8605131403')],separator='')\n",
                "aftertoken = mtokenizer.tokenize(Text.split())\n",
                "print(\"MWETokenizer\",aftertoken)\n",
                "aftertoken=(' '.join(aftertoken))\n",
                "print(aftertoken)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Sentence Tokenize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Mrunal Kulkarni\\nNavi Mumbai\\n9423627124 | maheshkulkarni01121@gmail.com\\nLinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni\\n\\nCareer Objective\\nTo learn and expand my knowledge in the field of computer science, particularly in the area of\\ndata science through which I can earn a job which enables me to use my passion for A.I to grow\\nthe company as well as accomplish my self-development goals.', 'Education Qualification\\n\\n● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with\\n\\naggregate of 9.37.', 'Sr.no\\n\\nQualification\\n\\n1\\n\\n2\\n\\n3\\n\\nB.tech\\n\\nHSC\\n\\nSSC\\n\\n9.58\\n\\n9.70\\n\\n9.48\\n\\n8.90\\n\\n9.14\\n\\n9.45\\n\\nGrade\\n\\nSem 1\\n\\nSem 2\\n\\nSem 3\\n\\nSem 4\\n\\nSem 5\\n\\nSem 6\\n\\n88.2%\\n\\n94.4%\\n\\nUniversity/board\\n\\nMumbai University\\n\\nCBSE\\n\\nCBSE\\n\\nTechnical Skills\\n\\n● Python programming skills\\n● Machine learning\\n● Natural Language Processing(BERT , Pegasus)\\n● C programming skills\\n● Web development (HTML,CSS,PHP)\\n\\n\\x0cProjects Done\\n\\n● Teams meeting summarization website using Pegasus model of Hugging Face\\n● Sentiment analysis using BERT\\n● Book recommendation website using Python,HTML\\n● Online Book review website\\n● Log book report generation website using PHP\\n\\nCourses Done\\n\\n● Fine tuning BERT for text classification using Python Tensorflow by Coursera\\n● Demystifying Networking course by IIT Bombay NPTEL\\n● Python Course by Kaggle\\n● C programming certificate by Spoken Tutorial\\n\\nInternships\\n\\n● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built\\na log book report generation website using PHP , Codeigniter using the existing database\\n\\nAchievements\\n\\n● Reached Semi-finals of Deep Blue Season 8\\n\\nExtra Curriculars\\n\\n● Content writing awards in College\\n● Video Editing for social media\\n\\nHobby\\n\\n● Story writing\\n● Reading\\n● Video editing\\n\\nPersonal Details\\n\\nName: Mrunal Mahesh Kulkarni\\nAddress: BARC colony ,Tarpur\\nPhone No: 9423627124\\nLanguages known: English, Hindi, Marathi\\n\\nDeclaration: I hereby declare that the above information is true to my knowledge.', 'Date: 3/8/23\\nPlace: New Panvel']\n"
                    ]
                }
            ],
            "source": [
                "from nltk.tokenize import sent_tokenize\n",
                "print(sent_tokenize(Text))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Filteration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def  filter_text(Text,lowerFlag=False,upperFlag=False,numberFlag=False,htmlFlag=False,urlFlag=False,punctFlag=False,spaceFlag=False,charFlag=False,bulletFlag=False,hashtagFlag=False,emojiFlag=False):\n",
                "    if lowerFlag:\n",
                "      Text = Text.lower()\n",
                "\n",
                "    if upperFlag:\n",
                "      Text = Text.upper()\n",
                "\n",
                "    if numberFlag:\n",
                "      import re\n",
                "      Text = re.sub(r\"\\d+\", '', Text)\n",
                "\n",
                "    if htmlFlag:\n",
                "      import re\n",
                "      Text = re.sub(r'<[^>]*>', '', Text)\n",
                "\n",
                "    if urlFlag:\n",
                "      import re\n",
                "      Text = re.sub(r'(https?|ftp|www)\\S+', '', Text)\n",
                "\n",
                "    if punctFlag:\n",
                "      import re\n",
                "      import string\n",
                "      exclist = string.punctuation #removes [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
                "      # remove punctuations and digits from oldtext\n",
                "      table_ = Text.maketrans('', '', exclist)\n",
                "      Text = Text.translate(table_)\n",
                "\n",
                "    if spaceFlag:\n",
                "      import re\n",
                "      Text = re.sub(' +',\" \",Text).strip()\n",
                "\n",
                "    if hashtagFlag:\n",
                "      import re\n",
                "      Text = re.sub(' +',\"#\",Text).strip()\n",
                "      \n",
                "    if charFlag:\n",
                "      import re\n",
                "      Text = re.sub('', ' ', Text)\n",
                "      \n",
                "    if bulletFlag:\n",
                "      import re\n",
                "      Text = re.sub('●','',Text)\n",
                "      Text = re.sub('★','',Text)\n",
                "      Text = re.sub('|','',Text)\n",
                "      # Text = re.sub(([0-9|A-Z]+[\\.|\\)]|)\\s+,'', Text)\n",
                "    \n",
                "    if emojiFlag:\n",
                "      # import emoji\n",
                "      Text = emoji.sub(' +',\"#\",Text).strip()\n",
                "      pass\n",
                "\n",
                "    return Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni Career Objective To learn and expand my knowledge in the field of computer science, particularly in the area of data science through which I can earn a job which enables me to use my passion for A.I to grow the company as well as accomplish my self-development goals. Education Qualification ● Pursuing B.Tech semester 7 in computer science in Pillai College of Engineering with aggregate of 9.37. Sr.no Qualification 1 2 3 B.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2% 94.4% University/board Mumbai University CBSE CBSE Technical Skills ● Python programming skills ● Machine learning ● Natural Language Processing(BERT , Pegasus) ● C programming skills ● Web development (HTML,CSS,PHP) Projects Done ● Teams meeting summarization website using Pegasus model of Hugging Face ● Sentiment analysis using BERT ● Book recommendation website using Python,HTML ● Online Book review website ● Log book report generation website using PHP Courses Done ● Fine tuning BERT for text classification using Python Tensorflow by Coursera ● Demystifying Networking course by IIT Bombay NPTEL ● Python Course by Kaggle ● C programming certificate by Spoken Tutorial Internships ● Completed one month training at Bhabha Atomic Research Center, Tarapur where I built a log book report generation website using PHP , Codeigniter using the existing database Achievements ● Reached Semi-finals of Deep Blue Season 8 Extra Curriculars ● Content writing awards in College ● Video Editing for social media Hobby ● Story writing ● Reading ● Video editing Personal Details Name: Mrunal Mahesh Kulkarni Address: BARC colony ,Tarpur Phone No: 9423627124 Languages known: English, Hindi, Marathi Declaration: I hereby declare that the above information is true to my knowledge. Date: 3/8/23 Place: New Panvel\n"
                    ]
                }
            ],
            "source": [
                "filteredtxt = filter_text(aftertoken)\n",
                "print(filteredtxt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Stopwords"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'be', 'didn', 'both', 'don', \"isn't\", 'is', 'myself', 'him', 'an', 'what', 'there', 'off', 'from', 'for', 'nor', 'which', 'so', \"won't\", \"haven't\", 'how', 'can', 'then', 'if', 'about', \"mustn't\", \"you're\", 'here', 'wouldn', 's', 'having', 'she', 'himself', 'our', \"it's\", 'being', 'with', 'them', 'above', 'very', 'than', \"shan't\", 'it', 'ours', 'me', \"should've\", \"she's\", 'up', 'you', 'he', 'a', 'do', 'mustn', \"that'll\", 'yourselves', 'ourselves', 'his', 'ain', 'when', 'more', 'that', 'now', 'they', 'does', 'herself', \"mightn't\", 'on', 'aren', 'too', \"you've\", 'some', 'all', 'this', 'only', 're', 'hadn', 'few', 'again', \"hadn't\", 't', 'who', 'the', 'did', 'because', 'by', 'm', \"don't\", 'we', 'before', 'weren', \"shouldn't\", 'hasn', 'as', 'into', 'those', 'through', 'why', 'own', 'down', 'between', 'shouldn', 'll', 'over', \"you'll\", 'each', 'ma', 'yours', 'was', 'once', 'but', 'o', 'hers', 'these', 'were', 'such', 'during', 'themselves', 'below', \"weren't\", 'at', 'am', 'no', 'her', 'other', 'doesn', 've', \"needn't\", 'not', \"aren't\", 'won', 'against', 'had', 'wasn', 'any', 'where', 'd', 'has', \"you'd\", 'in', 'my', 'itself', 'needn', \"wouldn't\", 'most', 'are', \"couldn't\", 'their', 'theirs', 'y', 'should', 'your', 'will', 'after', \"didn't\", 'further', 'isn', 'just', 'to', 'while', \"hasn't\", 'i', 'whom', 'until', 'yourself', 'its', 'couldn', \"wasn't\", 'of', 'shan', 'same', 'out', 'and', 'or', 'under', 'mightn', \"doesn't\", 'been', 'doing', 'haven', 'have'}\n"
                    ]
                }
            ],
            "source": [
                "# from nltk.corpus import stopwords\n",
                "stop_words = set(stopwords.words(\"english\"))\n",
                "print(stop_words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "StopWords recognized in the given sentence: ['is', 'for', 'which', 'can', 'with', 'above', 'me', 'a', 'that', 'the', 'by', 'as', 'through', 'at', 'where', 'in', 'my', 'to', 'of', 'and'] \n",
                        "After removing the recognized stopwords, the Tokens of sentence is: ['Mrunal', 'Kulkarni', 'Navi', 'Mumbai', '9423627124', '|', 'maheshkulkarni01121@gmail.com', 'LinkedIn:', 'www.linkedin.com/in/mrunal-mahesh-kulkarni', 'Career', 'Objective', 'To', 'learn', 'expand', 'knowledge', 'field', 'computer', 'science,', 'particularly', 'area', 'data', 'science', 'I', 'earn', 'job', 'enables', 'use', 'passion', 'A.I', 'grow', 'company', 'well', 'accomplish', 'self-development', 'goals.', 'Education', 'Qualification', '●', 'Pursuing', 'B.Tech', 'semester', '7', 'computer', 'science', 'Pillai', 'College', 'Engineering', 'aggregate', '9.37.', 'Sr.no', 'Qualification', '1', '2', '3', 'B.tech', 'HSC', 'SSC', '9.58', '9.70', '9.48', '8.90', '9.14', '9.45', 'Grade', 'Sem', '1', 'Sem', '2', 'Sem', '3', 'Sem', '4', 'Sem', '5', 'Sem', '6', '88.2%', '94.4%', 'University/board', 'Mumbai', 'University', 'CBSE', 'CBSE', 'Technical', 'Skills', '●', 'Python', 'programming', 'skills', '●', 'Machine', 'learning', '●', 'Natural', 'Language', 'Processing(BERT', ',', 'Pegasus)', '●', 'C', 'programming', 'skills', '●', 'Web', 'development', '(HTML,CSS,PHP)', 'Projects', 'Done', '●', 'Teams', 'meeting', 'summarization', 'website', 'using', 'Pegasus', 'model', 'Hugging', 'Face', '●', 'Sentiment', 'analysis', 'using', 'BERT', '●', 'Book', 'recommendation', 'website', 'using', 'Python,HTML', '●', 'Online', 'Book', 'review', 'website', '●', 'Log', 'book', 'report', 'generation', 'website', 'using', 'PHP', 'Courses', 'Done', '●', 'Fine', 'tuning', 'BERT', 'text', 'classification', 'using', 'Python', 'Tensorflow', 'Coursera', '●', 'Demystifying', 'Networking', 'course', 'IIT', 'Bombay', 'NPTEL', '●', 'Python', 'Course', 'Kaggle', '●', 'C', 'programming', 'certificate', 'Spoken', 'Tutorial', 'Internships', '●', 'Completed', 'one', 'month', 'training', 'Bhabha', 'Atomic', 'Research', 'Center,', 'Tarapur', 'I', 'built', 'log', 'book', 'report', 'generation', 'website', 'using', 'PHP', ',', 'Codeigniter', 'using', 'existing', 'database', 'Achievements', '●', 'Reached', 'Semi-finals', 'Deep', 'Blue', 'Season', '8', 'Extra', 'Curriculars', '●', 'Content', 'writing', 'awards', 'College', '●', 'Video', 'Editing', 'social', 'media', 'Hobby', '●', 'Story', 'writing', '●', 'Reading', '●', 'Video', 'editing', 'Personal', 'Details', 'Name:', 'Mrunal', 'Mahesh', 'Kulkarni', 'Address:', 'BARC', 'colony', ',Tarpur', 'Phone', 'No:', '9423627124', 'Languages', 'known:', 'English,', 'Hindi,', 'Marathi', 'Declaration:', 'I', 'hereby', 'declare', 'information', 'true', 'knowledge.', 'Date:', '3/8/23', 'Place:', 'New', 'Panvel']\n",
                        "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn: www.linkedin.com/in/mrunal-mahesh-kulkarni Career Objective To learn expand knowledge field computer science, particularly area data science I earn job enables use passion A.I grow company well accomplish self-development goals. Education Qualification ● Pursuing B.Tech semester 7 computer science Pillai College Engineering aggregate 9.37. Sr.no Qualification 1 2 3 B.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2% 94.4% University/board Mumbai University CBSE CBSE Technical Skills ● Python programming skills ● Machine learning ● Natural Language Processing(BERT , Pegasus) ● C programming skills ● Web development (HTML,CSS,PHP) Projects Done ● Teams meeting summarization website using Pegasus model Hugging Face ● Sentiment analysis using BERT ● Book recommendation website using Python,HTML ● Online Book review website ● Log book report generation website using PHP Courses Done ● Fine tuning BERT text classification using Python Tensorflow Coursera ● Demystifying Networking course IIT Bombay NPTEL ● Python Course Kaggle ● C programming certificate Spoken Tutorial Internships ● Completed one month training Bhabha Atomic Research Center, Tarapur I built log book report generation website using PHP , Codeigniter using existing database Achievements ● Reached Semi-finals Deep Blue Season 8 Extra Curriculars ● Content writing awards College ● Video Editing social media Hobby ● Story writing ● Reading ● Video editing Personal Details Name: Mrunal Mahesh Kulkarni Address: BARC colony ,Tarpur Phone No: 9423627124 Languages known: English, Hindi, Marathi Declaration: I hereby declare information true knowledge. Date: 3/8/23 Place: New Panvel\n"
                    ]
                }
            ],
            "source": [
                "mtokenizer = MWETokenizer([('athale20comp@studen', 't.mes.ac.in'), ('+91', '-', '8605131403')],separator='')\n",
                "word_tokens = mtokenizer.tokenize(filteredtxt.split())\n",
                "\n",
                "# print(ex_text)\n",
                "\n",
                "stop = [w for w in stop_words if w in word_tokens]\n",
                "print(\"StopWords recognized in the given sentence:\", stop,\"\")\n",
                "\n",
                "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
                "filtered_sentence = []\n",
                "\n",
                "for w in word_tokens:\n",
                "    if w not in stop_words:\n",
                "        filtered_sentence.append(w)\n",
                "\n",
                "print(\"After removing the recognized stopwords, the Tokens of sentence is:\", filtered_sentence)\n",
                "filtered_data=(' '.join(filtered_sentence))\n",
                "print(filtered_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Stemming or Lemmatization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Mrunal, Kulkarni, Navi, Mumbai, 9423627124, |, maheshkulkarni01121@gmail.com, LinkedIn, :, www.linkedin.com/in/mrunal-mahesh-kulkarni, Career, Objective, To, learn, expand, knowledge, field, computer, science, ,, particularly, area, data, science, I, earn, job, enables, use, passion, A.I, grow, company, well, accomplish, self, -, development, goals, ., Education, Qualification, ●, Pursuing, B.Tech, semester, 7, computer, science, Pillai, College, Engineering, aggregate, 9.37, ., Sr.no, Qualification, 1, 2, 3, B.tech, HSC, SSC, 9.58, 9.70, 9.48, 8.90, 9.14, 9.45, Grade, Sem, 1, Sem, 2, Sem, 3, Sem, 4, Sem, 5, Sem, 6, 88.2, %, 94.4, %, University, /, board, Mumbai, University, CBSE, CBSE, Technical, Skills, ●, Python, programming, skills, ●, Machine, learning, ●, Natural, Language, Processing(BERT, ,, Pegasus, ), ●, C, programming, skills, ●, Web, development, (, HTML, ,, CSS, ,, PHP, ), Projects, Done, ●, Teams, meeting, summarization, website, using, Pegasus, model, Hugging, Face, ●, Sentiment, analysis, using, BERT, ●, Book, recommendation, website, using, Python, ,, HTML, ●, Online, Book, review, website, ●, Log, book, report, generation, website, using, PHP, Courses, Done, ●, Fine, tuning, BERT, text, classification, using, Python, Tensorflow, Coursera, ●, Demystifying, Networking, course, IIT, Bombay, NPTEL, ●, Python, Course, Kaggle, ●, C, programming, certificate, Spoken, Tutorial, Internships, ●, Completed, one, month, training, Bhabha, Atomic, Research, Center, ,, Tarapur, I, built, log, book, report, generation, website, using, PHP, ,, Codeigniter, using, existing, database, Achievements, ●, Reached, Semi, -, finals, Deep, Blue, Season, 8, Extra, Curriculars, ●, Content, writing, awards, College, ●, Video, Editing, social, media, Hobby, ●, Story, writing, ●, Reading, ●, Video, editing, Personal, Details, Name, :, Mrunal, Mahesh, Kulkarni, Address, :, BARC, colony, ,, Tarpur, Phone, No, :, 9423627124, Languages, known, :, English, ,, Hindi, ,, Marathi, Declaration, :, I, hereby, declare, information, true, knowledge, ., Date, :, 3/8/23, Place, :, New, Panvel]\n",
                        "Mrunal Kulkarni Navi Mumbai 9423627124 | maheshkulkarni01121@gmail.com LinkedIn : www.linkedin.com/in/mrunal-mahesh-kulkarni career objective to learn expand knowledge field computer science , particularly area datum science I earn job enable use passion A.I grow company well accomplish self - development goal . Education Qualification ● pursue B.Tech semester 7 computer science Pillai College Engineering aggregate 9.37 . Sr.no Qualification 1 2 3 b.tech HSC SSC 9.58 9.70 9.48 8.90 9.14 9.45 Grade Sem 1 Sem 2 Sem 3 Sem 4 Sem 5 Sem 6 88.2 % 94.4 % University / board Mumbai University CBSE CBSE Technical Skills ● Python programming skill ● Machine learn ● Natural Language Processing(BERT , Pegasus ) ● c programming skill ● web development ( HTML , CSS , PHP ) project do ● Teams meet summarization website use Pegasus model Hugging Face ● Sentiment analysis use BERT ● Book recommendation website use Python , HTML ● Online Book review website ● Log book report generation website use PHP Courses done ● fine tune BERT text classification use Python Tensorflow Coursera ● Demystifying Networking course IIT Bombay NPTEL ● Python course Kaggle ● C programming certificate Spoken Tutorial Internships ● complete one month train Bhabha Atomic Research Center , Tarapur I build log book report generation website use PHP , Codeigniter use exist database Achievements ● Reached Semi - final Deep Blue Season 8 extra Curriculars ● Content write award College ● Video Editing social medium Hobby ● Story writing ● reading ● Video edit Personal Details Name : Mrunal Mahesh Kulkarni Address : barc colony , Tarpur Phone no : 9423627124 language know : English , Hindi , Marathi Declaration : I hereby declare information true knowledge . date : 3/8/23 place : New Panvel\n"
                    ]
                }
            ],
            "source": [
                "import spacy\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Create a Doc object\n",
                "doc = nlp(filtered_data)\n",
                "\n",
                "# Create list of tokens from given string\n",
                "tokens = []\n",
                "for token in doc:\n",
                "\ttokens.append(token)\n",
                "\n",
                "print(tokens)\n",
                "#> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n",
                "\n",
                "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
                "\n",
                "print(lemmatized_sentence)\n",
                "#> the bat see the cat with good stripe hang upside down by -PRON- foot\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "E-mail:  maheshkulkarni01121@gmail.com\n"
                    ]
                }
            ],
            "source": [
                "def extract_email(text):\n",
                "    '''\n",
                "    Helper function to extract email id from text\n",
                "\n",
                "    :param text: plain text extracted from resume file\n",
                "    '''\n",
                "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
                "    if email:\n",
                "        try:\n",
                "            return email[0].split()[0].strip(';')\n",
                "        except IndexError:\n",
                "            return None\n",
                "print(\"E-mail: \",extract_email(filtered_data))\n",
                "resume_data[titles[1]]=extract_email(filtered_data)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mobile No.:  9423627124\n"
                    ]
                }
            ],
            "source": [
                "def extract_mobile_number(text):\n",
                "    '''\n",
                "    Helper function to extract mobile number from text\n",
                "\n",
                "    :param text: plain text extracted from resume file\n",
                "    :return: string of extracted mobile numbers\n",
                "    '''\n",
                "    # Found this complicated regex on : https://zapier.com/blog/extract-links-email-phone-regex/\n",
                "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
                "    if phone:\n",
                "        number = ''.join(phone[0])\n",
                "        if len(number) > 10:\n",
                "            return '+' + number\n",
                "        else:\n",
                "            return number\n",
                "print(\"Mobile No.: \",extract_mobile_number(filtered_data))\n",
                "resume_data[titles[2]]=extract_mobile_number(filtered_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Analysis', 'Website', 'Video', 'Editing', 'Tensorflow', 'C', 'Php', 'Css', 'Engineering', 'Technical', 'Programming', 'Research', 'Networking', 'Content', 'Database', 'Python', 'English', 'Training', 'Writing', 'Html']\n"
                    ]
                }
            ],
            "source": [
                "def list_to_string(lst):\n",
                "    # Convert the list to a string representation\n",
                "    return ', '.join(map(str, lst))\n",
                "noun_chunks=[]\n",
                "extracted_skills=[]\n",
                "file='skills.csv'\n",
                "new_nlp = nlp(filtered_data)\n",
                "def extract_skills(nlp_text, noun_chunks):\n",
                "    '''\n",
                "    Helper function to extract skills from spacy nlp text\n",
                "\n",
                "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
                "    :param noun_chunks: noun chunks extracted from nlp text\n",
                "    :return: list of skills extracted\n",
                "    '''\n",
                "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
                "    data = pd.read_csv(os.path.join(os.path.dirname(file), 'skills.csv')) \n",
                "    skills = list(data.columns.values)\n",
                "    skillset = []\n",
                "    # check for one-grams\n",
                "    for token in tokens:\n",
                "        if token.lower() in skills:\n",
                "            skillset.append(token)\n",
                "    \n",
                "    # check for bi-grams and tri-grams\n",
                "    for token in noun_chunks:\n",
                "        token = token.text.lower().strip()\n",
                "        if token in skills:\n",
                "            skillset.append(token)\n",
                "    extracted_skills=[i.capitalize() for i in set([i.lower() for i in skillset])]\n",
                "\n",
                "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
                "extract_skills(new_nlp,noun_chunks)\n",
                "extracted_skills=extract_skills(new_nlp,noun_chunks)\n",
                "print(extracted_skills)\n",
                "resume_data[titles[5]]=list_to_string(extracted_skills)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import openpyxl\n",
                "from openpyxl import Workbook, load_workbook\n",
                "from datetime import datetime\n",
                "import pandas as pd\n",
                "\n",
                "import openpyxl as px\n",
                "\n",
                "# Specify the Excel file path\n",
                "excel_file = \"Extracted Data\\Extracted.xlsx\"\n",
                "\n",
                "# Load the existing Excel file\n",
                "try:\n",
                "    wb = px.load_workbook(excel_file)\n",
                "except FileNotFoundError:\n",
                "    # If the file doesn't exist, create a new workbook\n",
                "    wb = px.Workbook()\n",
                "\n",
                "# # Select the worksheet where you want to append data (or create a new one)\n",
                "# ws = wb.active  # You can also select a specific sheet by name: workbook[\"SheetName\"]\n",
                "\n",
                "# Get the current timestamp\n",
                "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
                "# setect the current active sheet\n",
                "ws = wb.active\n",
                "# check if the current sheet has same timestamp\n",
                "if ws.title == timestamp:\n",
                "\n",
                "    df = pd.DataFrame.from_dict([resume_data])\n",
                "    \n",
                "else:\n",
                "    # Create a new sheet with the timestamp as the name\n",
                "    ws = wb.create_sheet(title=timestamp, index=0)\n",
                "    ws[\"A1\"] = 'Email'\n",
                "    ws[\"B1\"] = 'Mobile No.'\n",
                "    ws[\"C1\"] = 'Skills'\n",
                "    df = pd.DataFrame.from_dict([resume_data])\n",
                "    \n",
                "      \n",
                "for index, row in df.iterrows():\n",
                "    ws.append(row.tolist())\n",
                "\n",
                "wb.save(excel_file)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Matching the extracted skills with our requirements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skills in Resume:  ['Analysis', 'Website', 'Video', 'Editing', 'Tensorflow', 'C', 'Php', 'Css', 'Engineering', 'Technical', 'Programming', 'Research', 'Networking', 'Content', 'Database', 'Python', 'English', 'Training', 'Writing', 'Html']\n",
                        "Required skills are:  ['engineering', 'html', 'c', 'java', 'javascript', 'php', 'python', 'machine learning', 'designing']\n",
                        "Matched skills are:  ['C', 'Php', 'Engineering', 'Python', 'Html']\n",
                        "The percentage of skills matched is:  55.55555555555556\n"
                    ]
                }
            ],
            "source": [
                "file='required.csv'\n",
                "new_nlp = nlp(filtered_data)\n",
                "from difflib import SequenceMatcher\n",
                "\n",
                "def match_skills(skills):\n",
                "    '''\n",
                "    Helper function to extract skills from spacy nlp text\n",
                "\n",
                "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
                "    :param noun_chunks: noun chunks extracted from nlp text\n",
                "    :return: list of skills extracted\n",
                "    '''\n",
                "    # tokens = [token.text for token in nlp_text if not token.is_stop]\n",
                "    # skills=[]\n",
                "    data = pd.read_csv(os.path.join(os.path.dirname(file), 'required.csv')) \n",
                "    required = list(data.columns.values)\n",
                "    print(\"Skills in Resume: \",skills)\n",
                "    print(\"Required skills are: \",required)\n",
                "    matched = []\n",
                "    # check for one-grams\n",
                "    for skill in skills:\n",
                "        if skill.lower() in required:\n",
                "            matched.append(skill)\n",
                "    print(\"Matched skills are: \",matched)\n",
                "    res = len(set(required) and set(matched)) / float(len(set(required) or set(matched))) * 100\n",
                "    print(\"The percentage of skills matched is: \",res)\n",
                "\n",
                "match_skills(extracted_skills)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Education Qualification:  ['x']\n"
                    ]
                }
            ],
            "source": [
                "def extract_education(nlp_text):\n",
                "    '''\n",
                "    Helper function to extract education from spacy nlp text\n",
                "\n",
                "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
                "    :return: tuple of education degree and year if year if found else only returns education degree\n",
                "    '''\n",
                "    EDUCATION         = [\n",
                "                    'CBSE', 'ICSE', 'X', 'XII', 'BE', 'B.E.', 'B.E', 'BS', 'B.S', 'ME', 'M.E', 'M.E.', 'MS', 'M.S', 'BTECH', 'MTECH', \n",
                "                    'SSC', 'HSC'\n",
                "                    ]\n",
                "    \n",
                "    STOPWORDS         = set(stopwords.words('english'))\n",
                "    \n",
                "    YEAR              = r'(((20|19)(\\d{2})))'\n",
                "    \n",
                "    edu = {}\n",
                "    # Extract education degree\n",
                "    for index, text in enumerate(nlp_text):\n",
                "        for tex in text.split():\n",
                "            # tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
                "            if tex.upper() in EDUCATION:\n",
                "                edu[tex] = text + nlp_text[index + 1]\n",
                "\n",
                "    # Extract year\n",
                "    education = []\n",
                "    for key in edu.keys():\n",
                "        year = re.search(re.compile(YEAR), edu[key])\n",
                "        if year:\n",
                "            education.append((key, ''.join(year.group(0))))\n",
                "        else:\n",
                "            education.append(key)\n",
                "    return education\n",
                "print(\"Education Qualification: \",extract_education(Text))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "ner using xlnet testing "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import XLNetForSequenceClassification, XLNetTokenizer, AdamW\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import json\n",
                "\n",
                "# Load your JSON dataset\n",
                "with open(\"traindata3.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
                "    data = json.load(json_file)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'content': 'Govardhana K\\nSenior Software Engineer\\n\\nBengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n\\nTotal IT experience 5 Years 6 Months\\nCloud Lending Solutions INC 4 Month • Salesforce Developer\\nOracle 5 Years 2 Month • Core Java Developer\\nLanguages Core Java, Go Lang\\nOracle PL-SQL programming,\\nSales Force Developer with APEX.\\n\\nDesignations & Promotions\\n\\nWilling to relocate: Anywhere\\n\\nWORK EXPERIENCE\\n\\nSenior Software Engineer\\n\\nCloud Lending Solutions -  Bangalore, Karnataka -\\n\\nJanuary 2018 to Present\\n\\nPresent\\n\\nSenior Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2016 to December 2017\\n\\nStaff Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nJanuary 2014 to October 2016\\n\\nAssociate Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2012 to December 2013\\n\\nEDUCATION\\n\\nB.E in Computer Science Engineering\\n\\nAdithya Institute of Technology -  Tamil Nadu\\n\\nSeptember 2008 to June 2012\\n\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nSKILLS\\n\\nAPEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n\\nLINKS\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Proficiency:\\n\\nLanguages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/',\n",
                            " 'annotation': [{'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 1749, 'end': 1754, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 1696, 'end': 1701, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 1417, 'end': 1422, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Skills'],\n",
                            "   'points': [{'start': 1356,\n",
                            "     'end': 1792,\n",
                            "     'text': 'Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 1209, 'end': 1214, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Skills'],\n",
                            "   'points': [{'start': 1136,\n",
                            "     'end': 1247,\n",
                            "     'text': 'APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n'}]},\n",
                            "  {'label': ['Graduation Year'],\n",
                            "   'points': [{'start': 928, 'end': 931, 'text': '2012'}]},\n",
                            "  {'label': ['College Name'],\n",
                            "   'points': [{'start': 858,\n",
                            "     'end': 888,\n",
                            "     'text': 'Adithya Institute of Technology'}]},\n",
                            "  {'label': ['Degree'],\n",
                            "   'points': [{'start': 821,\n",
                            "     'end': 855,\n",
                            "     'text': 'B.E in Computer Science Engineering'}]},\n",
                            "  {'label': ['Graduation Year'],\n",
                            "   'points': [{'start': 787, 'end': 790, 'text': '2012'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 744, 'end': 749, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Designation'],\n",
                            "   'points': [{'start': 722, 'end': 741, 'text': 'Associate Consultant'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 658, 'end': 663, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Designation'],\n",
                            "   'points': [{'start': 640, 'end': 655, 'text': 'Staff Consultant'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 574, 'end': 579, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Designation'],\n",
                            "   'points': [{'start': 555, 'end': 572, 'text': 'Senior Consultant\\n'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 470, 'end': 492, 'text': 'Cloud Lending Solutions'}]},\n",
                            "  {'label': ['Designation'],\n",
                            "   'points': [{'start': 444,\n",
                            "     'end': 468,\n",
                            "     'text': 'Senior Software Engineer\\n'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 308, 'end': 313, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 234, 'end': 239, 'text': 'Oracle'}]},\n",
                            "  {'label': ['Companies worked at'],\n",
                            "   'points': [{'start': 175, 'end': 197, 'text': 'Cloud Lending Solutions'}]},\n",
                            "  {'label': ['Email Address'],\n",
                            "   'points': [{'start': 93,\n",
                            "     'end': 136,\n",
                            "     'text': 'indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n'}]},\n",
                            "  {'label': ['Location'],\n",
                            "   'points': [{'start': 39, 'end': 47, 'text': 'Bengaluru'}]},\n",
                            "  {'label': ['Designation'],\n",
                            "   'points': [{'start': 13, 'end': 37, 'text': 'Senior Software Engineer\\n'}]},\n",
                            "  {'label': ['Name'],\n",
                            "   'points': [{'start': 0, 'end': 11, 'text': 'Govardhana K'}]}]}"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "data[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_label_and_points(label, points):\n",
                "    # Initialize an empty list to store the extracted entity texts\n",
                "    entity_texts = []\n",
                "    \n",
                "    # Loop through each point dictionary in the list\n",
                "    for point in points:\n",
                "        start = point['start']\n",
                "        end = point['end']\n",
                "        \n",
                "        # Extract the labeled entity text from the content\n",
                "        entity_text = content[start:end]\n",
                "        \n",
                "        # Append the entity text to the list\n",
                "        entity_texts.append(entity_text)\n",
                "    \n",
                "    # Combine the extracted entity texts if needed (e.g., join with a space)\n",
                "    combined_entity_text = ' '.join(entity_texts)\n",
                "    \n",
                "    return combined_entity_text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'input_data' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32mc:\\Users\\thale\\OneDrive\\Documents\\Avanti\\SEM 6\\resume analysis\\Resume-Analysis\\ResumeProcessing\\parser.ipynb Cell 34\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/Resume-Analysis/ResumeProcessing/parser.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_data\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'input_data' is not defined"
                    ]
                }
            ],
            "source": [
                "input_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import XLNetTokenizer\n",
                "import torch\n",
                "\n",
                "# Initialize the XLNet tokenizer\n",
                "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
                "\n",
                "# Tokenize and pad/truncate input sequences\n",
                "input_ids = []\n",
                "attention_masks = []\n",
                "\n",
                "for text in Text:\n",
                "    encoded_text = tokenizer.encode_plus(\n",
                "        text,\n",
                "        add_special_tokens=True,\n",
                "        max_length=128,  # Adjust the max length as needed\n",
                "        padding='max_length',\n",
                "        return_attention_mask=True,\n",
                "        return_tensors='pt'\n",
                "    )\n",
                "\n",
                "    input_ids.append(encoded_text['input_ids'])\n",
                "    attention_masks.append(encoded_text['attention_mask'])\n",
                "\n",
                "# Convert input_ids and attention_masks to tensors\n",
                "input_ids = torch.cat(input_ids, dim=0)\n",
                "attention_masks = torch.cat(attention_masks, dim=0)\n",
                "\n",
                "# Encode the target labels (NER tags) into tensors or other suitable format\n",
                "# You may need to use label encoders or other methods based on your model's requirements\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_ids"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "class NERDataset(Dataset):\n",
                "    def __init__(self, input_ids, attention_masks, target_labels):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            input_ids (list of torch.Tensor): List of input sequences as tensors.\n",
                "            attention_masks (list of torch.Tensor): List of attention masks as tensors.\n",
                "            target_labels (list of list of str): List of target labels for NER, where each list contains labels for one example.\n",
                "        \"\"\"\n",
                "        self.input_ids = input_ids\n",
                "        self.attention_masks = attention_masks\n",
                "        self.target_labels = target_labels\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        sample = {\n",
                "            'input_ids': self.input_ids[idx],\n",
                "            'attention_mask': self.attention_masks[idx],\n",
                "            'target_labels': self.target_labels[idx],\n",
                "        }\n",
                "        return sample\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import XLNetForTokenClassification, Trainer, TrainingArguments\n",
                "label_list = ['College Name', 'Designation', 'Name', 'Companies worked at', 'Email Address', 'Location', 'Skills']\n",
                "target_labels = [\n",
                "    [\"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", ],  # Labels for the first example\n",
                "    [\"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", ]  # Labels for the second example\n",
                "    # Add labels for more examples...\n",
                "]\n",
                "\n",
                "# Calculate the number of unique labels\n",
                "num_labels = len(label_list)# Load the pre-trained XLNet model for token classification\n",
                "model = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', num_labels=num_labels)  # Define num_labels accordingly\n",
                "\n",
                "# Define training arguments (batch size, learning rate, etc.)\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='./ner_model',\n",
                "    per_device_train_batch_size=16,\n",
                "    num_train_epochs=3,\n",
                "    learning_rate=2e-5,\n",
                ")\n",
                "\n",
                "# Initialize the Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=NERDataset(input_ids, attention_masks, target_labels),  # Define your custom dataset class\n",
                ")\n",
                "\n",
                "# Start training\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install transformers[torch]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting accelerate\n",
                        "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/d9/92/2d3aecf9f4a192968035880be3e2fc8b48d541c7128f7c936f430d6f96da/accelerate-0.23.0-py3-none-any.whl.metadata\n",
                        "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
                        "Requirement already satisfied: numpy>=1.17 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (1.22.3)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (21.3)\n",
                        "Requirement already satisfied: psutil in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (5.9.1)\n",
                        "Requirement already satisfied: pyyaml in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (6.0)\n",
                        "Requirement already satisfied: torch>=1.10.0 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (1.11.0)\n",
                        "Requirement already satisfied: huggingface-hub in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (0.4.0)\n",
                        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.7)\n",
                        "Requirement already satisfied: typing-extensions in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
                        "Requirement already satisfied: filelock in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub->accelerate) (3.6.0)\n",
                        "Requirement already satisfied: requests in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub->accelerate) (2.27.1)\n",
                        "Requirement already satisfied: tqdm in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub->accelerate) (4.63.0)\n",
                        "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.9)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2021.10.8)\n",
                        "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.12)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
                        "Requirement already satisfied: colorama in c:\\users\\thale\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->huggingface-hub->accelerate) (0.4.6)\n",
                        "Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
                        "   ---------------------------------------- 258.1/258.1 kB 2.7 MB/s eta 0:00:00\n",
                        "Installing collected packages: accelerate\n",
                        "Successfully installed accelerate-0.23.0\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "DEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
                        "\n",
                        "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "pip install accelerate -U\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.Collecting spark-nlp\n",
                        "  Obtaining dependency information for spark-nlp from https://files.pythonhosted.org/packages/cd/7d/bc0eca4c9ec4c9c1d9b28c42c2f07942af70980a7d912d0aceebf8db32dd/spark_nlp-5.1.3-py2.py3-none-any.whl.metadata\n",
                        "  Downloading spark_nlp-5.1.3-py2.py3-none-any.whl.metadata (53 kB)\n",
                        "     ---------------------------------------- 54.0/54.0 kB 2.9 MB/s eta 0:00:00\n",
                        "Downloading spark_nlp-5.1.3-py2.py3-none-any.whl (537 kB)\n",
                        "   ---------------------------------------- 537.5/537.5 kB 3.4 MB/s eta 0:00:00\n",
                        "Installing collected packages: spark-nlp\n",
                        "Successfully installed spark-nlp-5.1.3\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "DEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
                        "\n",
                        "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "pip install spark-nlp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "^C\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install pyspark\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install -q transformers tensorflow sentencepiece"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import XLNetTokenizer, TFXLNetModel\n",
                "import tensorflow as tf\n",
                "\n",
                "# xlnet-base-cased\n",
                "MODEL_NAME = 'xlnet-base-cased'\n",
                "\n",
                "XLNetTokenizer.from_pretrained(MODEL_NAME, return_tensors=\"pt\").save_pretrained(\"./{}_tokenizer\".format(MODEL_NAME))\n",
                "\n",
                "# just in case if there is no TF/Keras file provided in the model\n",
                "# we can just use `from_pt` and convert PyTorch to TensorFlow\n",
                "try:\n",
                "  print('try downloading TF weights')\n",
                "  model = TFXLNetModel.from_pretrained(MODEL_NAME)\n",
                "except:\n",
                "  print('try downloading PyTorch weights')\n",
                "  model = TFXLNetModel.from_pretrained(MODEL_NAME, from_pt=True)\n",
                "\n",
                "model.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sparknlp\n",
                "# let's start Spark with Spark NLP\n",
                "spark = sparknlp.start()\n",
                "\n",
                "from sparknlp.annotator import *\n",
                "\n",
                "xlnet = XlnetEmbeddings.loadSavedModel(\n",
                "     '{}/saved_model/1'.format(MODEL_NAME),\n",
                "     spark\n",
                " )\\\n",
                " .setInputCols([\"sentence\",'token'])\\\n",
                " .setOutputCol(\"embeddings\")\\\n",
                " .setCaseSensitive(True)\\\n",
                " .setDimension(768)\\\n",
                " .setStorageRef('xlnet_base_cased') "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install -U huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install transformers --upgrade"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ffcbd08877704edd80f414f660cf8f00",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32mc:\\Users\\Mrunal Kulkarni\\OneDrive\\Documents\\GitHub\\Resume-Analysis\\ResumeProcessing\\parser.ipynb Cell 48\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mxlnet-base-cased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m XLNetForTokenClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mxlnet-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m,cache_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./cache\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMy name is Mrunal\u001b[39m\u001b[39m\"\u001b[39m, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mrunal%20Kulkarni/OneDrive/Documents/GitHub/Resume-Analysis/ResumeProcessing/parser.ipynb#X65sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2929\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2926\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2927\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 2929\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2930\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs\n\u001b[0;32m   2931\u001b[0m         )\n\u001b[0;32m   2932\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   2933\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   2934\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   2935\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2936\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   2937\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   2938\u001b[0m     )\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1431\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1431\u001b[0m     http_get(\n\u001b[0;32m   1432\u001b[0m         url_to_download,\n\u001b[0;32m   1433\u001b[0m         temp_file,\n\u001b[0;32m   1434\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1435\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1436\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1437\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1438\u001b[0m     )\n\u001b[0;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1441\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    541\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    544\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    545\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    550\u001b[0m )\n\u001b[1;32m--> 551\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    553\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
                        "File \u001b[1;32mc:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, XLNetForTokenClassification\n",
                "import torch\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
                "model = XLNetForTokenClassification.from_pretrained('xlnet-base-cased',cache_dir='./cache')\n",
                "\n",
                "inputs = tokenizer(\n",
                "    \"My name is Mrunal\", add_special_tokens=False, return_tensors=\"pt\"\n",
                ")\n",
                "\n",
                "with torch.no_grad():\n",
                "    logits = model(**inputs).logits\n",
                "\n",
                "predicted_token_class_ids = logits.argmax(-1)\n",
                "\n",
                "# Note that tokens are classified rather then input words which means that\n",
                "# there might be more predicted token classes than words.\n",
                "# Multiple token classes might account for the same word\n",
                "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
                "\n",
                "labels = predicted_token_class_ids\n",
                "loss = model(**inputs, labels=labels).loss\n",
                "labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, XLNetForTokenClassification\n",
                "import torch\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
                "model = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', cache_dir='./cache')\n",
                "\n",
                "inputs = tokenizer(\n",
                "    \"My name is Mrunal\", add_special_tokens=False, return_tensors=\"pt\"\n",
                ")\n",
                "\n",
                "with torch.no_grad():\n",
                "    logits = model(**inputs).logits\n",
                "\n",
                "predicted_token_class_ids = logits.argmax(-1)\n",
                "\n",
                "# Note that tokens are classified rather than input words, which means that\n",
                "# there might be more predicted token classes than words.\n",
                "# Multiple token classes might account for the same word\n",
                "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
                "\n",
                "# Create a mapping for class labels to integer IDs\n",
                "unique_labels = list(set(predicted_tokens_classes))\n",
                "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
                "\n",
                "# Convert class labels to integer IDs\n",
                "label_ids = [label_to_id[label] for label in predicted_tokens_classes]\n",
                "\n",
                "# Now, label_ids contains the integer IDs corresponding to the predicted class labels\n",
                "print(label_ids)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4b09b8449fc145dea34dbb443ef42888",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Mrunal Kulkarni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mrunal Kulkarni\\OneDrive\\Documents\\GitHub\\Resume-Analysis\\ResumeProcessing\\cache. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
                        "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
                        "  warnings.warn(message)\n",
                        "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, XLNetForTokenClassification\n",
                "import torch\n",
                "\n",
                "# Load tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
                "model = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', cache_dir='./cache')\n",
                "\n",
                "# Define your input text\n",
                "input_text = \"My name is Mrunal and I work at Google in California.\"\n",
                "\n",
                "# Tokenize the input text\n",
                "tokens = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
                "\n",
                "# Make predictions using the model\n",
                "with torch.no_grad():\n",
                "    logits = model(**tokens).logits\n",
                "\n",
                "# Get the predicted token class IDs\n",
                "predicted_token_class_ids = logits.argmax(-1)[0].tolist()\n",
                "\n",
                "# Get the corresponding token labels using the model's configuration\n",
                "predicted_labels = [model.config.id2label[class_id] for class_id in predicted_token_class_ids]\n",
                "\n",
                "# Convert the tokenized input text into a list of tokens\n",
                "tokens = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0].tolist())\n",
                "\n",
                "# Combine consecutive tokens with the same label into named entities\n",
                "named_entities = []\n",
                "current_entity = {\"text\": \"\", \"label\": None}\n",
                "for token, label in zip(tokens, predicted_labels):\n",
                "    if label != 'O':\n",
                "        if current_entity[\"label\"] is None:\n",
                "            current_entity[\"label\"] = label\n",
                "        current_entity[\"text\"] += token.replace('▁', ' ')\n",
                "    else:\n",
                "        if current_entity[\"label\"] is not None:\n",
                "            named_entities.append(current_entity)\n",
                "            current_entity = {\"text\": \"\", \"label\": None}\n",
                "\n",
                "# Print the recognized named entities\n",
                "for entity in named_entities:\n",
                "    print(f\"Entity: {entity['text']}, Label: {entity['label']}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of XLNetForTokenClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens: {'input_ids': tensor([[ 631,  304,   27,  414, 2169,  212,   21,   35,  154,   38, 2964,   25,\n",
                        "          848,    9]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
                        "Predicted Token Class IDs: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
                        "Predicted Labels: ['LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n",
                        "Named Entities: []\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, XLNetForTokenClassification\n",
                "import torch\n",
                "\n",
                "# Load tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
                "model = XLNetForTokenClassification.from_pretrained('xlnet-base-cased', cache_dir='./cache')\n",
                "\n",
                "# Define your input text\n",
                "input_text = \"My name is Mrunal and I work at Google in California.\"\n",
                "\n",
                "# Tokenize the input text\n",
                "tokens = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
                "\n",
                "# Make predictions using the model\n",
                "with torch.no_grad():\n",
                "    logits = model(**tokens).logits\n",
                "\n",
                "# Get the predicted token class IDs\n",
                "predicted_token_class_ids = logits.argmax(-1)[0].tolist()\n",
                "\n",
                "# Get the corresponding token labels using the model's configuration\n",
                "predicted_labels = [model.config.id2label[class_id] for class_id in predicted_token_class_ids]\n",
                "\n",
                "# Print tokenization and predictions for debugging\n",
                "print(\"Tokens:\", tokens)\n",
                "print(\"Predicted Token Class IDs:\", predicted_token_class_ids)\n",
                "print(\"Predicted Labels:\", predicted_labels)\n",
                "\n",
                "# Post-processing to extract named entities\n",
                "named_entities = []\n",
                "current_entity = {\"text\": \"\", \"label\": None}\n",
                "for token, label in zip(tokens, predicted_labels):\n",
                "    if label != 'O':\n",
                "        if current_entity[\"label\"] is None:\n",
                "            current_entity[\"label\"] = label\n",
                "        current_entity[\"text\"] += token.replace('▁', ' ')\n",
                "    else:\n",
                "        if current_entity[\"label\"] is not None:\n",
                "            named_entities.append(current_entity)\n",
                "            current_entity = {\"text\": \"\", \"label\": None}\n",
                "\n",
                "# Print the recognized named entities for debugging\n",
                "print(\"Named Entities:\", named_entities)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
                        "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
                "import torch\n",
                "\n",
                "# Load tokenizer and NER model\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
                "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
                "\n",
                "# Define your input text\n",
                "input_text = \"My name is John and I work at Google in California.\"\n",
                "\n",
                "# Tokenize the input text\n",
                "tokens = tokenizer(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
                "\n",
                "# Make predictions using the NER model\n",
                "with torch.no_grad():\n",
                "    logits = model(**tokens).logits\n",
                "\n",
                "# Get the predicted token class IDs\n",
                "predicted_token_class_ids = logits.argmax(-1)[0].tolist()\n",
                "\n",
                "# Get the corresponding token labels using the model's configuration\n",
                "predicted_labels = [model.config.id2label[class_id] for class_id in predicted_token_class_ids]\n",
                "\n",
                "# Print the recognized named entities\n",
                "named_entities = []\n",
                "current_entity = {\"text\": \"\", \"label\": None}\n",
                "for token, label in zip(tokens, predicted_labels):\n",
                "    if label != 'O':\n",
                "        if current_entity[\"label\"] is None:\n",
                "            current_entity[\"label\"] = label\n",
                "        current_entity[\"text\"] += token.replace('▁', ' ')\n",
                "    else:\n",
                "        if current_entity[\"label\"] is not None:\n",
                "            named_entities.append(current_entity)\n",
                "            current_entity = {\"text\": \"\", \"label\": None}\n",
                "\n",
                "# Print the recognized named entities\n",
                "for entity in named_entities:\n",
                "    print(f\"Entity: {entity['text']}, Label: {entity['label']}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
                        "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
                        "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "\n",
                "# create pipeline for NER\n",
                "ner = pipeline('ner', aggregation_strategy = 'simple')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'entity_group': 'PER',\n",
                            "  'score': 0.98419696,\n",
                            "  'word': 'Sakshi Sunil Patil',\n",
                            "  'start': 0,\n",
                            "  'end': 18},\n",
                            " {'entity_group': 'ORG',\n",
                            "  'score': 0.89909315,\n",
                            "  'word': 'Pillai College',\n",
                            "  'start': 29,\n",
                            "  'end': 43},\n",
                            " {'entity_group': 'LOC',\n",
                            "  'score': 0.76000166,\n",
                            "  'word': 'Navi Mumbai',\n",
                            "  'start': 59,\n",
                            "  'end': 70}]"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "ner(\"Sakshi Sunil Patil 703007244 Pillai College of engineering Navi Mumbai\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
                "from transformers import pipeline\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
                "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
                "\n",
                "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "example = \"John smith 703007244 Pillai College of engineering Navi Mumbai\"\n",
                "\n",
                "ner_results = nlp(example)\n",
                "ner_results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "tokenizer2 = AutoTokenizer.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\n",
                "model2 = AutoModelForTokenClassification.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\n",
                "nlp2 = pipeline(\"ner\", model=model2, tokenizer=tokenizer2, aggregation_strategy=\"max\")\n",
                "\n",
                "example = \"My name is Mrunal Mahesh Kulkarni and I work at Google at Navi Mumbai\"\n",
                "ner_results2 = nlp2(example)\n",
                "ner_results2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
