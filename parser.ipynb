{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the libraries\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WordPunctTokenizer, MWETokenizer\n",
    "# import PyPDF2\n",
    "# for pdf to txt\n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import TextConverter\n",
    "from io import StringIO\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "import docx2txt\n",
    "import constants as cs\n",
    "import string\n",
    "import utils\n",
    "import pprint\n",
    "from spacy.matcher import matcher\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from constants import STOPWORDS\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_file=input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "print(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract txt from pdf\n",
    "# pdf_path=\"code\\Resume.pdf\"\n",
    "def pdf_to_txt(pdf_path):\n",
    "    resource_manager = PDFResourceManager(caching=True)\n",
    "    \n",
    "    # create a string object that will contain the final text the representation of the pdf. \n",
    "    out_text = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laParams = LAParams()\n",
    "    \n",
    "    # Create a TextConverter Object:\n",
    "    text_converter = TextConverter(resource_manager, out_text, laparams=laParams)\n",
    "    fp = open(pdf_path, 'rb')\n",
    "    #Create a PDF interpreter object\n",
    "\n",
    "    interpreter = PDFPageInterpreter(resource_manager, text_converter)\n",
    "    \n",
    "    # We are going to process the content of each page of the original PDF File    \n",
    "    for page in PDFPage.get_pages(fp, pagenos=set(), maxpages=0, password=\"\", caching=True, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    \n",
    "    # Retrieve the entire contents of the “file” at any time \n",
    "    text = out_text.getvalue()\n",
    "\n",
    "    # Closing all the ressources we previously opened\n",
    "\n",
    "    fp.close()\n",
    "    text_converter.close()\n",
    "    out_text.close()\n",
    "    \n",
    "    return text\n",
    "# retext = pdf_to_txt(pdf_path)\n",
    "# print(pdf_to_txt(pdf_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract txt from\n",
    "# docx_path=\"Resume.docx\"\n",
    "def docx_to_txt(docx_path):\n",
    "    temp = docx2txt.process(docx_path)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)\n",
    "# print(docx_to_txt(docx_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Resume', '.pdf')\n",
      "File Name:  Resume\n",
      "File Extension:  .pdf\n"
     ]
    }
   ],
   "source": [
    "split_tup = os.path.splitext(resume_file)\n",
    "print(split_tup)\n",
    "  \n",
    "# extract the file name and extension\n",
    "file_name = split_tup[0]\n",
    "file_extension = split_tup[1]\n",
    "  \n",
    "print(\"File Name: \", file_name)\n",
    "print(\"File Extension: \", file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avanti Makarand Thale\n",
      "\n",
      "athale20comp@studen\n",
      "t.mes.ac.in\n",
      "+91 - 8605131403\n",
      "\n",
      "CAREER OBJECTIVE\n",
      "\n",
      "To obtain a challenging position in software design while\n",
      "\n",
      "contributing signiﬁcantly to the company's growth.\n",
      "\n",
      "EDUCATION QUALIFICATION\n",
      "\n",
      "● University Of Mumbai, Mumbai — Graduation\n",
      "\n",
      "AUG 2020 – PRESENT\n",
      "\n",
      "B.Tech in Computer Science with Specialization in Data\n",
      "\n",
      "Science and Natural Language Processing through Pillai\n",
      "\n",
      "College Of Engineering, Panvel\n",
      "\n",
      "● Jindal Mount Litera Zee School, Nagothane —\n",
      "\n",
      "H.S.C\n",
      "\n",
      "MAR 2020\n",
      "\n",
      "scored 84% in XII Science CBSE in (PCMB)\n",
      "\n",
      "● Jindal Vidya Mandir Salav, Murud — S.S.C\n",
      "\n",
      "MAR 2018\n",
      "\n",
      "Scored 94% in class X CBSE\n",
      "\n",
      "SKILLS\n",
      "\n",
      "★ Leadership\n",
      "\n",
      "★ Problem Solving\n",
      "\n",
      "★ Teamwork\n",
      "\n",
      "PROGRAMMING\n",
      "LANGUAGES\n",
      "\n",
      "★ Python\n",
      "\n",
      "★ Java\n",
      "\n",
      "★ Javascript\n",
      "\n",
      "★ HTML\n",
      "\n",
      "★ CSS\n",
      "\n",
      "★ C\n",
      "\n",
      "★ Django\n",
      "\n",
      "INTERESTS\n",
      "\n",
      "★ Listening music\n",
      "\n",
      "★ Reading Books\n",
      "\n",
      "\fACHIEVEMENTS\n",
      "\n",
      "● Project Deep Blue\n",
      "\n",
      "Participated in Project Deep Blue as the Leader of the\n",
      "\n",
      "team and worked on the solution for Summarizing the\n",
      "\n",
      "teams meeting\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "● Diabetes Detection System\n",
      "\n",
      "using Python, Flask Application using (SVM) Support\n",
      "\n",
      "Vector Machine Algorithm\n",
      "\n",
      "● Book Review Site\n",
      "\n",
      "using CSS, Html, Javascript, PHP\n",
      "\n",
      "● Railway Reservation System\n",
      "\n",
      "using Python Flask\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "# detect file extension and call above functions accordingly\n",
    "def extract_text(file_path, extension):\n",
    "    '''\n",
    "    Wrapper function to detect the file extension and call text extraction function accordingly\n",
    "\n",
    "    :param file_path: path of file of which text is to be extracted\n",
    "    :param extension: extension of file `file_name`\n",
    "    '''\n",
    "    text = ''\n",
    "    if extension == '.pdf':\n",
    "        for page in pdf_to_txt(file_path):\n",
    "            text += '' + page\n",
    "    elif extension == '.docx' or extension == '.doc':\n",
    "        text = docx_to_txt(file_path)\n",
    "    return text\n",
    "Text= extract_text(resume_file,file_extension)\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Of the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\thale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize ['Avanti', 'Makarand', 'Thale', 'athale20comp', '@', 'studen', 't.mes.ac.in', '+91', '-', '8605131403', 'CAREER', 'OBJECTIVE', 'To', 'obtain', 'a', 'challenging', 'position', 'in', 'software', 'design', 'while', 'contributing', 'signiﬁcantly', 'to', 'the', 'company', \"'s\", 'growth', '.', 'EDUCATION', 'QUALIFICATION', '●', 'University', 'Of', 'Mumbai', ',', 'Mumbai', '—', 'Graduation', 'AUG', '2020', '–', 'PRESENT', 'B.Tech', 'in', 'Computer', 'Science', 'with', 'Specialization', 'in', 'Data', 'Science', 'and', 'Natural', 'Language', 'Processing', 'through', 'Pillai', 'College', 'Of', 'Engineering', ',', 'Panvel', '●', 'Jindal', 'Mount', 'Litera', 'Zee', 'School', ',', 'Nagothane', '—', 'H.S.C', 'MAR', '2020', 'scored', '84', '%', 'in', 'XII', 'Science', 'CBSE', 'in', '(', 'PCMB', ')', '●', 'Jindal', 'Vidya', 'Mandir', 'Salav', ',', 'Murud', '—', 'S.S.C', 'MAR', '2018', 'Scored', '94', '%', 'in', 'class', 'X', 'CBSE', 'SKILLS', '★', 'Leadership', '★', 'Problem', 'Solving', '★', 'Teamwork', 'PROGRAMMING', 'LANGUAGES', '★', 'Python', '★', 'Java', '★', 'Javascript', '★', 'HTML', '★', 'CSS', '★', 'C', '★', 'Django', 'INTERESTS', '★', 'Listening', 'music', '★', 'Reading', 'Books', 'ACHIEVEMENTS', '●', 'Project', 'Deep', 'Blue', 'Participated', 'in', 'Project', 'Deep', 'Blue', 'as', 'the', 'Leader', 'of', 'the', 'team', 'and', 'worked', 'on', 'the', 'solution', 'for', 'Summarizing', 'the', 'teams', 'meeting', 'PROJECTS', '●', 'Diabetes', 'Detection', 'System', 'using', 'Python', ',', 'Flask', 'Application', 'using', '(', 'SVM', ')', 'Support', 'Vector', 'Machine', 'Algorithm', '●', 'Book', 'Review', 'Site', 'using', 'CSS', ',', 'Html', ',', 'Javascript', ',', 'PHP', '●', 'Railway', 'Reservation', 'System', 'using', 'Python', 'Flask']\n",
      "TreebankWordTokenizer ['Avanti', 'Makarand', 'Thale', 'athale20comp', '@', 'studen', 't.mes.ac.in', '+91', '-', '8605131403', 'CAREER', 'OBJECTIVE', 'To', 'obtain', 'a', 'challenging', 'position', 'in', 'software', 'design', 'while', 'contributing', 'signiﬁcantly', 'to', 'the', 'company', \"'s\", 'growth.', 'EDUCATION', 'QUALIFICATION', '●', 'University', 'Of', 'Mumbai', ',', 'Mumbai', '—', 'Graduation', 'AUG', '2020', '–', 'PRESENT', 'B.Tech', 'in', 'Computer', 'Science', 'with', 'Specialization', 'in', 'Data', 'Science', 'and', 'Natural', 'Language', 'Processing', 'through', 'Pillai', 'College', 'Of', 'Engineering', ',', 'Panvel', '●', 'Jindal', 'Mount', 'Litera', 'Zee', 'School', ',', 'Nagothane', '—', 'H.S.C', 'MAR', '2020', 'scored', '84', '%', 'in', 'XII', 'Science', 'CBSE', 'in', '(', 'PCMB', ')', '●', 'Jindal', 'Vidya', 'Mandir', 'Salav', ',', 'Murud', '—', 'S.S.C', 'MAR', '2018', 'Scored', '94', '%', 'in', 'class', 'X', 'CBSE', 'SKILLS', '★', 'Leadership', '★', 'Problem', 'Solving', '★', 'Teamwork', 'PROGRAMMING', 'LANGUAGES', '★', 'Python', '★', 'Java', '★', 'Javascript', '★', 'HTML', '★', 'CSS', '★', 'C', '★', 'Django', 'INTERESTS', '★', 'Listening', 'music', '★', 'Reading', 'Books', 'ACHIEVEMENTS', '●', 'Project', 'Deep', 'Blue', 'Participated', 'in', 'Project', 'Deep', 'Blue', 'as', 'the', 'Leader', 'of', 'the', 'team', 'and', 'worked', 'on', 'the', 'solution', 'for', 'Summarizing', 'the', 'teams', 'meeting', 'PROJECTS', '●', 'Diabetes', 'Detection', 'System', 'using', 'Python', ',', 'Flask', 'Application', 'using', '(', 'SVM', ')', 'Support', 'Vector', 'Machine', 'Algorithm', '●', 'Book', 'Review', 'Site', 'using', 'CSS', ',', 'Html', ',', 'Javascript', ',', 'PHP', '●', 'Railway', 'Reservation', 'System', 'using', 'Python', 'Flask']\n",
      "WordPunctTokenizer ['Avanti', 'Makarand', 'Thale', 'athale20comp', '@', 'studen', 't', '.', 'mes', '.', 'ac', '.', 'in', '+', '91', '-', '8605131403', 'CAREER', 'OBJECTIVE', 'To', 'obtain', 'a', 'challenging', 'position', 'in', 'software', 'design', 'while', 'contributing', 'signiﬁcantly', 'to', 'the', 'company', \"'\", 's', 'growth', '.', 'EDUCATION', 'QUALIFICATION', '●', 'University', 'Of', 'Mumbai', ',', 'Mumbai', '—', 'Graduation', 'AUG', '2020', '–', 'PRESENT', 'B', '.', 'Tech', 'in', 'Computer', 'Science', 'with', 'Specialization', 'in', 'Data', 'Science', 'and', 'Natural', 'Language', 'Processing', 'through', 'Pillai', 'College', 'Of', 'Engineering', ',', 'Panvel', '●', 'Jindal', 'Mount', 'Litera', 'Zee', 'School', ',', 'Nagothane', '—', 'H', '.', 'S', '.', 'C', 'MAR', '2020', 'scored', '84', '%', 'in', 'XII', 'Science', 'CBSE', 'in', '(', 'PCMB', ')', '●', 'Jindal', 'Vidya', 'Mandir', 'Salav', ',', 'Murud', '—', 'S', '.', 'S', '.', 'C', 'MAR', '2018', 'Scored', '94', '%', 'in', 'class', 'X', 'CBSE', 'SKILLS', '★', 'Leadership', '★', 'Problem', 'Solving', '★', 'Teamwork', 'PROGRAMMING', 'LANGUAGES', '★', 'Python', '★', 'Java', '★', 'Javascript', '★', 'HTML', '★', 'CSS', '★', 'C', '★', 'Django', 'INTERESTS', '★', 'Listening', 'music', '★', 'Reading', 'Books', 'ACHIEVEMENTS', '●', 'Project', 'Deep', 'Blue', 'Participated', 'in', 'Project', 'Deep', 'Blue', 'as', 'the', 'Leader', 'of', 'the', 'team', 'and', 'worked', 'on', 'the', 'solution', 'for', 'Summarizing', 'the', 'teams', 'meeting', 'PROJECTS', '●', 'Diabetes', 'Detection', 'System', 'using', 'Python', ',', 'Flask', 'Application', 'using', '(', 'SVM', ')', 'Support', 'Vector', 'Machine', 'Algorithm', '●', 'Book', 'Review', 'Site', 'using', 'CSS', ',', 'Html', ',', 'Javascript', ',', 'PHP', '●', 'Railway', 'Reservation', 'System', 'using', 'Python', 'Flask']\n"
     ]
    }
   ],
   "source": [
    "#word tokenize\n",
    "print(\"word_tokenize\",word_tokenize(Text))\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(\"TreebankWordTokenizer\",tokenizer.tokenize(Text))\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(\"WordPunctTokenizer\",tokenizer.tokenize(Text) )\n",
    "\n",
    "# from nltk.tokenize import MWETokenizer\n",
    "# tokenizer = MWETokenizer([('New', 'Panvel'), ('Kohann', 'K.', 'Toper')],separator=' ')\n",
    "\n",
    "# print(\"MWETokenizer\",tokenizer.tokenize(Text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Avanti Makarand Thale\\n\\nathale20comp@studen\\nt.mes.ac.in\\n+91 - 8605131403\\n\\nCAREER OBJECTIVE\\n\\nTo obtain a challenging position in software design while\\n\\ncontributing signiﬁcantly to the company's growth.\", 'EDUCATION QUALIFICATION\\n\\n● University Of Mumbai, Mumbai — Graduation\\n\\nAUG 2020 – PRESENT\\n\\nB.Tech in Computer Science with Specialization in Data\\n\\nScience and Natural Language Processing through Pillai\\n\\nCollege Of Engineering, Panvel\\n\\n● Jindal Mount Litera Zee School, Nagothane —\\n\\nH.S.C\\n\\nMAR 2020\\n\\nscored 84% in XII Science CBSE in (PCMB)\\n\\n● Jindal Vidya Mandir Salav, Murud — S.S.C\\n\\nMAR 2018\\n\\nScored 94% in class X CBSE\\n\\nSKILLS\\n\\n★ Leadership\\n\\n★ Problem Solving\\n\\n★ Teamwork\\n\\nPROGRAMMING\\nLANGUAGES\\n\\n★ Python\\n\\n★ Java\\n\\n★ Javascript\\n\\n★ HTML\\n\\n★ CSS\\n\\n★ C\\n\\n★ Django\\n\\nINTERESTS\\n\\n★ Listening music\\n\\n★ Reading Books\\n\\n\\x0cACHIEVEMENTS\\n\\n● Project Deep Blue\\n\\nParticipated in Project Deep Blue as the Leader of the\\n\\nteam and worked on the solution for Summarizing the\\n\\nteams meeting\\n\\nPROJECTS\\n\\n● Diabetes Detection System\\n\\nusing Python, Flask Application using (SVM) Support\\n\\nVector Machine Algorithm\\n\\n● Book Review Site\\n\\nusing CSS, Html, Javascript, PHP\\n\\n● Railway Reservation System\\n\\nusing Python Flask']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1433660568.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [117]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Text = re.sub(([0-9|A-Z]+[\\.|\\)]|)\\s+,'',Text)\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "def  filter_text(Text,lowerFlag=False,upperFlag=False,numberFlag=False,htmlFlag=False,urlFlag=False,punctFlag=False,spaceFlag=False,charFlag=False,bulletFlag=False,hashtagFlag=False,emojiFlag=False):\n",
    "    if lowerFlag:\n",
    "      Text = Text.lower()\n",
    "\n",
    "    if upperFlag:\n",
    "      Text = Text.upper()\n",
    "\n",
    "    if numberFlag:\n",
    "      import re\n",
    "      Text = re.sub(r\"\\d+\", '', Text)\n",
    "\n",
    "    if htmlFlag:\n",
    "      import re\n",
    "      Text = re.sub(r'<[^>]*>', '', Text)\n",
    "\n",
    "    if urlFlag:\n",
    "      import re\n",
    "      Text = re.sub(r'(https?|ftp|www)\\S+', '', Text)\n",
    "\n",
    "    if punctFlag:\n",
    "      import re\n",
    "      import string\n",
    "      exclist = string.punctuation #removes [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "      # remove punctuations and digits from oldtext\n",
    "      table_ = Text.maketrans('', '', exclist)\n",
    "      Text = Text.translate(table_)\n",
    "\n",
    "    if spaceFlag:\n",
    "      import re\n",
    "      Text = re.sub(' +',\" \",Text).strip()\n",
    "\n",
    "    if hashtagFlag:\n",
    "      import re\n",
    "      Text = re.sub(' +',\"#\",Text).strip()\n",
    "      \n",
    "    if charFlag:\n",
    "      import re\n",
    "      Text = re.sub('\\n', ' ', Text)\n",
    "      \n",
    "    if bulletFlag:\n",
    "      import re\n",
    "      Text = re.sub('●','',Text)\n",
    "      Text = re.sub('★','',Text)\n",
    "      # Text = re.sub(([0-9|A-Z]+[\\.|\\)]|)\\s+,'', Text)\n",
    "    \n",
    "    if emojiFlag:s\n",
    "      import emoji\n",
    "      Text = emoji.sub(' +',\"#\",Text).strip()\n",
    "      pass\n",
    "\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avanti Makarand Thale  athale20comp@studen t.mes.ac.in +91 - 8605131403  CAREER OBJECTIVE  To obtain a challenging position in software design while  contributing signiﬁcantly to the company's growth.  EDUCATION QUALIFICATION   University Of Mumbai, Mumbai — Graduation  AUG 2020 – PRESENT  B.Tech in Computer Science with Specialization in Data  Science and Natural Language Processing through Pillai  College Of Engineering, Panvel   Jindal Mount Litera Zee School, Nagothane —  H.S.C  MAR 2020  scored 84% in XII Science CBSE in (PCMB)   Jindal Vidya Mandir Salav, Murud — S.S.C  MAR 2018  Scored 94% in class X CBSE  SKILLS   Leadership   Problem Solving   Teamwork  PROGRAMMING LANGUAGES   Python   Java   Javascript   HTML   CSS   C   Django  INTERESTS   Listening music   Reading Books  \fACHIEVEMENTS   Project Deep Blue  Participated in Project Deep Blue as the Leader of the  team and worked on the solution for Summarizing the  teams meeting  PROJECTS   Diabetes Detection System  using Python, Flask Application using (SVM) Support  Vector Machine Algorithm   Book Review Site  using CSS, Html, Javascript, PHP   Railway Reservation System  using Python Flask  \f\n"
     ]
    }
   ],
   "source": [
    "filteredtxt = filter_text(Text, bulletFlag=True, charFlag=True)\n",
    "print(filteredtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'mustn', 'from', 'they', 'why', 'my', 'will', 'just', 'herself', 'him', 'of', 'up', 'can', 'weren', \"you'll\", 'such', 'with', 'not', \"isn't\", 'is', 'shan', 'you', 'haven', 'do', 'only', \"didn't\", 'on', 'now', 'ain', 'be', 'hers', 'by', 'that', 'more', 'them', 'again', 'own', 'then', 'those', 'under', 're', 'once', 'theirs', 've', 'yourselves', \"you're\", 'myself', 'so', 'have', 'wouldn', 'has', 'its', 'same', 'at', 'needn', 'into', 'over', 'didn', 'because', \"mustn't\", 'it', 'our', \"hadn't\", 'what', 'll', 'mightn', 'than', 'during', 'off', 'their', 'while', 'if', 'how', 'should', 's', \"don't\", \"should've\", 'his', 'am', \"she's\", 'out', 'being', 'through', 'shouldn', 'when', 'aren', 'your', 'below', \"it's\", 'he', 'i', 'other', 'couldn', 'doesn', 'which', 'having', 'itself', 'ours', 'yourself', \"shouldn't\", 'before', 'down', 'was', 'here', 'isn', 'very', 'most', \"hasn't\", \"wouldn't\", 'themselves', 'me', 'these', 'doing', 'a', 'between', 'to', \"you'd\", \"wasn't\", 'ma', 'both', \"haven't\", \"that'll\", 'had', \"needn't\", 'the', 'in', 'each', 'but', \"doesn't\", 'does', 'until', 'she', 'where', 'whom', 'any', \"you've\", 'and', 'hasn', 'all', \"couldn't\", \"aren't\", 't', 'some', 'wasn', 'above', 'few', \"won't\", 'y', \"shan't\", \"weren't\", 'there', 'no', 'against', 'after', 'yours', 'an', 'himself', 'ourselves', 'further', 'd', 'her', 'who', 'm', 'nor', 'we', 'did', 'were', 'or', 'about', 'too', 'been', \"mightn't\", 'hadn', 'won', 'this', 'as', 'o', 'for', 'are'}\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopWords recognized in the given sentence: ['of', 'with', 'on', 'while', 'through', 'a', 'to', 'the', 'in', 'and', 'as', 'for'] \n",
      "\n",
      "After removing the recognized stopwords, the Tokens of sentence is: ['Avanti', 'Makarand', 'Thale', 'athale20comp', '@', 'studen', 't.mes.ac.in', '+91', '-', '8605131403', 'CAREER', 'OBJECTIVE', 'To', 'obtain', 'challenging', 'position', 'software', 'design', 'contributing', 'signiﬁcantly', 'company', \"'s\", 'growth', '.', 'EDUCATION', 'QUALIFICATION', 'University', 'Of', 'Mumbai', ',', 'Mumbai', '—', 'Graduation', 'AUG', '2020', '–', 'PRESENT', 'B.Tech', 'Computer', 'Science', 'Specialization', 'Data', 'Science', 'Natural', 'Language', 'Processing', 'Pillai', 'College', 'Of', 'Engineering', ',', 'Panvel', 'Jindal', 'Mount', 'Litera', 'Zee', 'School', ',', 'Nagothane', '—', 'H.S.C', 'MAR', '2020', 'scored', '84', '%', 'XII', 'Science', 'CBSE', '(', 'PCMB', ')', 'Jindal', 'Vidya', 'Mandir', 'Salav', ',', 'Murud', '—', 'S.S.C', 'MAR', '2018', 'Scored', '94', '%', 'class', 'X', 'CBSE', 'SKILLS', 'Leadership', 'Problem', 'Solving', 'Teamwork', 'PROGRAMMING', 'LANGUAGES', 'Python', 'Java', 'Javascript', 'HTML', 'CSS', 'C', 'Django', 'INTERESTS', 'Listening', 'music', 'Reading', 'Books', 'ACHIEVEMENTS', 'Project', 'Deep', 'Blue', 'Participated', 'Project', 'Deep', 'Blue', 'Leader', 'team', 'worked', 'solution', 'Summarizing', 'teams', 'meeting', 'PROJECTS', 'Diabetes', 'Detection', 'System', 'using', 'Python', ',', 'Flask', 'Application', 'using', '(', 'SVM', ')', 'Support', 'Vector', 'Machine', 'Algorithm', 'Book', 'Review', 'Site', 'using', 'CSS', ',', 'Html', ',', 'Javascript', ',', 'PHP', 'Railway', 'Reservation', 'System', 'using', 'Python', 'Flask']\n",
      "Avanti Makarand Thale athale20comp @ studen t.mes.ac.in +91 - 8605131403 CAREER OBJECTIVE To obtain challenging position software design contributing signiﬁcantly company 's growth . EDUCATION QUALIFICATION University Of Mumbai , Mumbai — Graduation AUG 2020 – PRESENT B.Tech Computer Science Specialization Data Science Natural Language Processing Pillai College Of Engineering , Panvel Jindal Mount Litera Zee School , Nagothane — H.S.C MAR 2020 scored 84 % XII Science CBSE ( PCMB ) Jindal Vidya Mandir Salav , Murud — S.S.C MAR 2018 Scored 94 % class X CBSE SKILLS Leadership Problem Solving Teamwork PROGRAMMING LANGUAGES Python Java Javascript HTML CSS C Django INTERESTS Listening music Reading Books ACHIEVEMENTS Project Deep Blue Participated Project Deep Blue Leader team worked solution Summarizing teams meeting PROJECTS Diabetes Detection System using Python , Flask Application using ( SVM ) Support Vector Machine Algorithm Book Review Site using CSS , Html , Javascript , PHP Railway Reservation System using Python Flask\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(filteredtxt)\n",
    "\n",
    "# print(ex_text)\n",
    "\n",
    "stop = [w for w in stop_words if w in word_tokens]\n",
    "print(\"StopWords recognized in the given sentence:\", stop,\"\\n\")\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(\"After removing the recognized stopwords, the Tokens of sentence is:\", filtered_sentence)\n",
    "filtered_data=(' '.join(filtered_sentence))\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thale\\OneDrive\\Documents\\Avanti\\SEM 6\\resume analysis\\code\\nlp\\parser.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/code/nlp/parser.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/code/nlp/parser.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/code/nlp/parser.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Create a Doc object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thale/OneDrive/Documents/Avanti/SEM%206/resume%20analysis/code/nlp/parser.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp(u,filtered_data)\n",
      "File \u001b[1;32mc:\\Users\\thale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u,filtered_data)\n",
    "\n",
    "# Create list of tokens from given string\n",
    "tokens = []\n",
    "for token in doc:\n",
    "\ttokens.append(token)\n",
    "\n",
    "print(tokens)\n",
    "#> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n",
    "\n",
    "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "print(lemmatized_sentence)\n",
    "#> the bat see the cat with good stripe hang upside down by -PRON- foot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_entity_sections(text):\n",
    "#     '''\n",
    "#     Helper function to extract all the raw text from sections of resume\n",
    "\n",
    "#     :param text: Raw text of resume\n",
    "#     :return: dictionary of entities\n",
    "#     '''\n",
    "#     text_split = [i.strip() for i in text.split('\\n')]\n",
    "#     # sections_in_resume = [i for i in text_split if i.lower() in sections]\n",
    "#     entities = {}\n",
    "#     key = False\n",
    "#     for phrase in text_split:\n",
    "#         if len(phrase) == 1:\n",
    "#             p_key = phrase\n",
    "#         else:\n",
    "#             p_key = set(phrase.lower().split()) & set(cs.RESUME_SECTIONS)\n",
    "#         try:\n",
    "#             p_key = list(p_key)[0]\n",
    "#         except IndexError:\n",
    "#             pass\n",
    "#         if p_key in cs.RESUME_SECTIONS:\n",
    "#             entities[p_key] = []\n",
    "#             key = p_key\n",
    "#         elif key and phrase.strip():\n",
    "#             entities[key].append(phrase)\n",
    "    \n",
    "#     # entity_key = False\n",
    "#     # for entity in entities.keys():\n",
    "#     #     sub_entities = {}\n",
    "#     #     for entry in entities[entity]:\n",
    "#     #         if u'\\u2022' not in entry:\n",
    "#     #             sub_entities[entry] = []\n",
    "#     #             entity_key = entry\n",
    "#     #         elif entity_key:\n",
    "#     #             sub_entities[entity_key].append(entry)\n",
    "#     #     entities[entity] = sub_entities\n",
    "\n",
    "#     # pprint.pprint(entities)\n",
    "\n",
    "#     # make entities that are not found None\n",
    "#     # for entity in cs.RESUME_SECTIONS:\n",
    "#     #     if entity not in entities.keys():\n",
    "#     #         entities[entity] = None \n",
    "#     return entities\n",
    "# # print(extract_entity_sections(retext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_email(text):\n",
    "#     '''\n",
    "#     Helper function to extract email id from text\n",
    "\n",
    "#     :param text: plain text extracted from resume file\n",
    "#     '''\n",
    "#     email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "#     if email:\n",
    "#         try:\n",
    "#             return email[0].split()[0].strip(';')\n",
    "#         except IndexError:\n",
    "#             return None\n",
    "# # print(extract_email(retext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_name(text, matcher):\n",
    "#     '''\n",
    "#     Helper function to extract name from spacy nlp text\n",
    "\n",
    "#     :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "#     :param matcher: object of `spacy.matcher.Matcher`\n",
    "#     :return: string of full name\n",
    "#     '''\n",
    "#     pattern = [cs.NAME_PATTERN]\n",
    "    \n",
    "#     matcher.add('NAME', None, *pattern)\n",
    "    \n",
    "#     matches = matcher(text)\n",
    "    \n",
    "#     for match_id, start, end in matches:\n",
    "#         span = text[start:end]\n",
    "#         return span.text\n",
    "# # print(extract_name(retext, matcher))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
